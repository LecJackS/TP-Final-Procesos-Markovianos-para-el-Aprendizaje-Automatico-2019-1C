{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Quick Roadmap to\n",
    "# Asynchronous Advantage Actor Critic (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![input-to-nn-channel-combined](./img/snaps-0-1-2-3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximación de Functiones (*Function Approximation*)\n",
    "\n",
    "Hasta ahora hemos visto distintas maneras de encontrar soluciones a **Procesos de Decisión Markovianos** (***MDPs***) del tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/agent-env-diagram.png\" align=\"center\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello, un agente puede aprender y/o hacer uso de\n",
    "- una ***Value Function*** / Función de valor\n",
    "- una ***Policy Function*** / Política de comportamiento\n",
    "- o un ***Modelo*** del Environment / Donde *modelo* será conocer las ***transiciones*** del environment (o un estimado de las mismas)\n",
    "\n",
    "Estas funciones, son esencialmente un *mapeo* (*mapping*) que **reducen la dimensionalidad** del problema: \n",
    "\n",
    "---\n",
    "\n",
    "> desde $\\big[$ `observación` $\\big]$ $\\mapsto$ $\\big[$ `representación con \"mayor nivel de abstracción\"` $\\big]$\n",
    "   \n",
    "---\n",
    "#### Particularmente:\n",
    "\n",
    "- Value Function:\n",
    "\n",
    "> desde $\\big[$ `estados o pares estado-acción` $\\big]$ $\\mapsto$ $\\big[$ `algún valor / puntaje` $\\big]$\n",
    "---\n",
    "- Policy Function:\n",
    "\n",
    "> desde $\\big[$ `estados` $\\big]$ $\\mapsto$ $\\big[$ `acciones` $\\big]$\n",
    ">\n",
    "> desde $\\big[$ `states` $\\big]$ $\\mapsto$ $\\big[$ `actions` $\\big]$\n",
    "\n",
    "---\n",
    "- Modelo: \n",
    "\n",
    "> desde $\\big[$ `estado` $\\big]$ $\\mapsto$ $\\big[$ `siguiente observación o esperanza de` $\\big]$\n",
    "\n",
    "---\n",
    "Queremos aprender estas funciones **directamente desde la experiencia**.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "\n",
    "Una opción intuitiva sería usar **Funciones de Valor**, ya que son fáciles de interpretar y de decidir una política a partir de ellas, además de disponer de una teoría más extensa que soporte sus prácticas.\n",
    "\n",
    "### Tabular\n",
    "\n",
    "Los algoritmos más simples, se basan en **soluciones tabulares**, que buscan llevar un **historial** de los estados visitados hasta el momento.\n",
    "\n",
    "El problema con ésto es que la cantidad de estados posible es gigantesca a pesar de estar resolviendo problemas simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T18:49:30.163529Z",
     "start_time": "2019-07-07T18:49:30.026042Z"
    }
   },
   "source": [
    "![pacman-micro-layout](./img/pacman-micro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality\n",
    "\n",
    "Consideremos un mapa (*layout*) de *pac-man* de 5x5 celdas como el de arriba.\n",
    "\n",
    "![pacman-micro-grid](./img/pacman-micro-grid.png)\n",
    "\n",
    "Si contamos las diferentes formas de organizar **solo** las bolitas (*pellets*), existen $2^{25}$ (más de *33 millones*) posibles combinaciones, que para modelos tabulares serán **estados completamente diferentes** uno de otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulación\n",
    "\n",
    "Llevadolo a la práctica, usando el algoritmo ***Q Learning*** para resolver una versión aún más simple de este mapa (con solo 2 pellets) se necesitan alrededor de **2000 episodios** para aprender a ganar casi con seguridad, pero la cantidad de estados (*states*) guardados en la Q function supera los **8000 elementos** (*<state: value>*), con una fuerte tendencia a seguir aumentando, mientras que el reward promedio se mantiene casi constante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T19:10:28.814430Z",
     "start_time": "2019-07-07T19:10:28.688116Z"
    }
   },
   "source": [
    "Entrenado por 2000 episodios (juegos):\n",
    "![pacman-2000-ep-plot](./img/q-table-2000-ep.png \"2000 episodes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de 10 mil episodios:\n",
    "\n",
    "![pacman-10000-ep-plot](./img/q-table-10000-ep.png  \"10000 episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes celdas permiten **reproducir los resultados** de arriba al entrenar un agente con *Q learning* por 2000 episodios, y mostrar 3 juegos siguiendo una política *greedy* con respecto a la Value Function aproximada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:09:35.694093Z",
     "start_time": "2019-07-14T02:09:35.689689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original directory:\n",
      " /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/\n",
      "Moved to:\n",
      " /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman\n"
     ]
    }
   ],
   "source": [
    "# Change directory to run pac-man simulator python files\n",
    "# Assumes notebook on main directory\n",
    "import os\n",
    "ORIGINAL_DIR = None\n",
    "if not ORIGINAL_DIR:\n",
    "    ORIGINAL_DIR = os.getcwd()\n",
    "try:\n",
    "    cs188x_pacman_dir = './cs188x_pacman'\n",
    "    os.chdir(cs188x_pacman_dir)\n",
    "    original_dir = os.getcwd()[:2-len(cs188x_pacman_dir)]\n",
    "    print(\"Original directory:\\n\",original_dir)\n",
    "    print(\"Moved to:\\n\", os.getcwd())\n",
    "except:\n",
    "    print(\"Couldn't change to ./cs188x_pacman folder. Are you already there?\")\n",
    "    print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:09:37.147493Z",
     "start_time": "2019-07-14T02:09:36.956086Z"
    }
   },
   "outputs": [],
   "source": [
    "import util\n",
    "import game\n",
    "import pacman\n",
    "\n",
    "from pacman import readCommand, runGames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:04:13.371690Z",
     "start_time": "2019-07-14T02:03:29.146318Z"
    },
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 2000 episodes of Training\n",
      "Episodes\tMean R lifetime / last 50 ep\tTraining time\n",
      "  50/2000\tr: -510.30 / -510.30\t0.33 sec\n",
      " 100/2000\tr: -501.08 / -491.86\t0.45 sec\n",
      " 150/2000\tr: -504.41 / -511.08\t0.57 sec\n",
      " 200/2000\tr: -506.61 / -513.18\t0.62 sec\n",
      " 250/2000\tr: -495.34 / -450.28\t0.65 sec\n",
      " 300/2000\tr: -450.89 / -228.64\t0.61 sec\n",
      " 350/2000\tr: -439.30 / -369.74\t0.61 sec\n",
      " 400/2000\tr: -428.12 / -349.84\t0.67 sec\n",
      " 450/2000\tr: -417.38 / -331.52\t0.65 sec\n",
      " 500/2000\tr: -402.79 / -271.44\t0.68 sec\n",
      " 550/2000\tr: -390.78 / -270.70\t0.75 sec\n",
      " 600/2000\tr: -381.02 / -273.70\t0.99 sec\n",
      " 650/2000\tr: -366.42 / -191.20\t0.88 sec\n",
      " 700/2000\tr: -349.65 / -131.56\t1.01 sec\n",
      " 750/2000\tr: -332.43 / -91.44\t0.84 sec\n",
      " 800/2000\tr: -316.19 / -72.48\t0.85 sec\n",
      " 850/2000\tr: -300.57 / -50.76\t0.83 sec\n",
      " 900/2000\tr: -283.35 / 9.36\t0.83 sec\n",
      " 950/2000\tr: -270.14 / -32.22\t0.87 sec\n",
      "1000/2000\tr: -250.13 / 129.90\t0.85 sec\n",
      "1050/2000\tr: -237.86 / 7.70\t0.87 sec\n",
      "1100/2000\tr: -230.34 / -72.40\t0.96 sec\n",
      "1150/2000\tr: -217.39 / 67.50\t0.96 sec\n",
      "1200/2000\tr: -206.29 / 48.94\t0.91 sec\n",
      "1250/2000\tr: -200.07 / -50.92\t0.83 sec\n",
      "1300/2000\tr: -194.46 / -54.02\t0.89 sec\n",
      "1350/2000\tr: -186.98 / 7.54\t0.87 sec\n",
      "1400/2000\tr: -173.46 / 191.38\t0.81 sec\n",
      "1450/2000\tr: -162.34 / 149.08\t0.95 sec\n",
      "1500/2000\tr: -158.09 / -34.86\t0.93 sec\n",
      "1550/2000\tr: -148.11 / 151.32\t0.81 sec\n",
      "1600/2000\tr: -141.93 / 49.56\t0.82 sec\n",
      "1650/2000\tr: -139.20 / -51.90\t0.83 sec\n",
      "1700/2000\tr: -130.70 / 149.98\t0.86 sec\n",
      "1750/2000\tr: -123.29 / 128.62\t0.90 sec\n",
      "1800/2000\tr: -114.01 / 210.88\t0.83 sec\n",
      "1850/2000\tr: -109.10 / 67.72\t0.89 sec\n",
      "1900/2000\tr: -102.82 / 129.46\t0.85 sec\n",
      "1950/2000\tr: -99.95 / 9.14\t0.81 sec\n",
      "2000/2000\tr: -95.25 / 88.02\t0.90 sec\n",
      "Training Done (turning off epsilon and alpha)\n",
      "---------------------------------------------\n",
      "Plotting Q size over episodes:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q size (table): 8803 different states saved\n",
      "\n",
      "Pacman emerges victorious! Score: 495\n",
      "Pacman died! Score: -516\n",
      "Pacman emerges victorious! Score: 499\n",
      "Average Score: 159.33333333333334\n",
      "Scores:        495.0, -516.0, 499.0\n",
      "Win Rate:      2/3 (0.67)\n",
      "Record:        Win, Loss, Win\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<game.Game at 0x7f287a310080>,\n",
       " <game.Game at 0x7f287a3103c8>,\n",
       " <game.Game at 0x7f287a4164a8>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "prefs = ['--pacman',     'PacmanQAgent',\n",
    "         '--numTraining','2000',\n",
    "         '--numGames',   '2003',\n",
    "         '--layout',     'smallGrid']\n",
    "\n",
    "args = readCommand(prefs)\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only Test\n",
    "\n",
    "El siguiente código simula 3 juegos con un agente **previamente entrenado** por 2000 episodios (solo *Testing*)\n",
    "\n",
    "El fantasma es estocástico, por lo que **cada juego será diferente**, por más que el agente mantenga en memoria siempre los mismos pares <estado, acción>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:18:45.863993Z",
     "start_time": "2019-07-13T19:18:34.560059Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^ Comment this line to get a verbose output\n",
    "prefs = ['--pacman',     'PacmanQAgent',\n",
    "         '--numTraining','0',\n",
    "         '--numGames',   '3',\n",
    "         '--layout',     'smallGrid',\n",
    "         '--replay',     'recorded/recorded-game-pacman-q-agent-2000th-game']\n",
    "args = readCommand(prefs)  # Get game components based on prefs\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[qLearningAgents.py](./cs188x_pacman/qlearningAgents.py) contiene la implementación de los distintos agentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:29:20.211225Z",
     "start_time": "2019-07-13T18:29:20.209109Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# To record EVERY game, add: , ['--recordActions', 'True']\n",
    "#prefs = ['--pacman',        'PacmanQAgent',\n",
    "#         '--numTraining',   '2000',\n",
    "#         '--numGames',      '2003',\n",
    "#         '--layout',        'smallGrid',\n",
    "#         '--recordActions', 'True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:19:37.416230Z",
     "start_time": "2019-07-13T19:19:37.413949Z"
    }
   },
   "outputs": [],
   "source": [
    "# To print all options use '--help'\n",
    "#args = pacman.readCommand(['--help']) # or uncomment this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación de Función de Valor\n",
    "\n",
    "Vemos que hay varios problemas de modelos tabulares para grandes MDPs: \n",
    "\n",
    "- Necesitamos **enormes cantidades de memoria** para guardar cada estado posible\n",
    "- Aprender un valor **para cada** estado requiere de **mucho tiempo**\n",
    "- Generalmente los estados son **parcialmente observables**\n",
    "\n",
    "Queda claro que para resolver problemas con **grandes espacios** de estado (discreto, continuos e *infinitos*), será necesario **aproximar la Función de Valor**, y no depender de recordar cada observación.\n",
    "\n",
    "Para estados similares, esperamos obtener resultados similares.\n",
    "\n",
    "Si el espacio de estados es muy grande o contínuo, sólo veremos cada estado una única vez, por lo que sólo recordarlos no nos ayudará a predecir algún otro parecido.\n",
    "___\n",
    "\n",
    "Siendo concretos, en el ejemplo de Pac-man\n",
    "\n",
    "* Si en el pasado obtuvimos un ***reward negativo*** por estar al lado de un fantasma en la esquina superior del mapa\n",
    "  \n",
    "  queremos que si encuentra un estado *\"parecido\"* lo considere peligroso (**generalice**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pacman-danger-similar-states](./img/pacman-micro-danger1.png  \"Similar states\")\n",
    "\n",
    "$$\\text{Dos estados completamente diferentes para modelos tabulares}$$\n",
    "\n",
    "$$\\text{Diferencia: pellet desplazado dos celdas a la derecha}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen distintos tipos de aproximadores de funciones que podemos usar, como:\n",
    "\n",
    "* **Aproximador lineal** (si conocemos *buenas features*)\n",
    "* **Red Neuronal**\n",
    "* Decision tree\n",
    "* Neirest Neighbour (no paramétrico)\n",
    "* Constructores de features\n",
    "  * Fourier / wavelets bases\n",
    "  * Coarse coding\n",
    "\n",
    "Pero necesitamos considerar propiedades particulares de Reinforcement Learning:\n",
    "* La **experiencia no es iid** (steps consecutivos están altamente correlacionados)\n",
    "* La **política** del agente afecta **directamente la data** que recibe\n",
    "* Las Funciones de Valor pueden ser **no-estacionarias** (como al usar *bootstraping*)\n",
    "* El feedback **no** suele ser instantáneo (*delayed feedback*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximador Lineal\n",
    "\n",
    "Es la más simple de las opciones de arriba, pero tiene **el mayor peso teórico** que la respalda, lo que la vuelve una excelente opción para una gran variedad de casos.\n",
    "\n",
    "Se basa en reducir el espacio de estados a una serie de *features* que lo representan, y usar como **Función de Valor aproximada**, a una combinación lineal de estas *features* con un cierto peso $w_i$ (*weights*) que se necesita aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n $ : número de *features*\n",
    "\n",
    "$f_i(s,a)$ : función que devuelve un valor correspondiente a una feature $i$ dado estado $s$ y acción $a$\n",
    "\n",
    "$$\\tilde{Q}(s,a) = \\sum\\limits_{i=1}^n f_i(s,a) * w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte de la simpleza de esta herramienta, surge de que al derivar nuestra función Q aproximada con respecto a los pesos, obtenemos una sumatoria de **solo los valores de las *features***, lo que facilita actualizar nuestras estimaciones de los distintos pesos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error de predicción** (o *advantage function*):\n",
    "\n",
    "$advantage = (R + \\gamma \\max\\limits_{a} \\tilde{Q}(S', a)) - \\tilde{Q}(S,A)$\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$w_i \\leftarrow w_i + \\alpha \\cdot advantage \\cdot f_i(S,A)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a nuestro simulador de Pacman, podemos resolver desafíos con un **espacio mucho mayor**, en una **menor cantidad de episodios y espacio en memoria** (aunque **mayor cantidad de tiempo** de aprendizaje), usando un aproximador lineal con features como:\n",
    "\n",
    "1. Fantasma en celda contigua (1 ó 0)\n",
    "2. Comer *pellet* (1 ó 0)\n",
    "3. Distancia a comida más cercana (float)\n",
    "\n",
    "ver archivo [./cs188x_pacman/featureExtractors.py](./cs188x_pacman/featureExtractors.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código entrena por 100 partidas un aproximador lineal con las features mencionadas en un mapa mucho mayor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:10:30.037655Z",
     "start_time": "2019-07-14T02:09:53.563847Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 100 episodes of Training\n",
      "Episodes\tMean R lifetime / last 50 ep\tTraining time\n",
      "  50/ 100\tr: 1049.46 / 1049.46\t9.75 sec\n",
      " 100/ 100\tr: 1065.04 / 1080.62\t9.13 sec\n",
      "Training Done (turning off epsilon and alpha)\n",
      "---------------------------------------------\n",
      "Pacman emerges victorious! Score: 1332\n",
      "Average Score: 1332.0\n",
      "Scores:        1332.0\n",
      "Win Rate:      1/1 (1.00)\n",
      "Record:        Win\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<game.Game at 0x7f5aa29eebe0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pacman import readCommand, runGames\n",
    "prefs = ['--pacman',     'ApproximateQAgent',\n",
    "         '--numTraining','100',\n",
    "         '--numGames',   '101',\n",
    "         '--layout',     'mediumClassic',\n",
    "         '--agentArgs',  'extractor=SimpleExtractor']\n",
    "args = readCommand(prefs)  # Get game components based on prefs\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se facilita (solo como visualización) un agente con aproximador lineal **ya entrenado** por 100 episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T19:21:50.305911Z",
     "start_time": "2019-07-13T19:21:28.877667Z"
    },
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^ Comment this line to get a verbose output\n",
    "prefs = ['--pacman',     'ApproximateQAgent',\n",
    "         '--numTraining','0',\n",
    "         '--numGames',   '1',\n",
    "         '--layout',     'mediumClassic',\n",
    "         '--agentArgs',  'extractor=SimpleExtractor',\n",
    "         '--replay',     'recorded/recorded-game-pacman-approx-linear-q-agent-100th-game']\n",
    "args = readCommand(prefs)  # Get game components based on options\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas con usar aproximadores lineales, es que necesitamos ***buenas* features** para obtener resultados agradables, donde \"*buenas*\" dependerá de cada caso particular.\n",
    "\n",
    "En el ejemplo de arriba, \"*Fantasma en celda contigua*\" aumenta en gran medida la performance del algoritmo, lo que intuitivamente es de esperar, ya que estamos casi **prefijando** el comportamiento de pacman para este problema en particular.\n",
    "\n",
    "Y aún consiguiendo estas features, vienen con la limitación de que en un aproximador lineal, no podrán ser \"relacionadas\" entre sí, por lo que debemos incluirlas como productos o algún otro tipo de polinomio, dejando de ser lineal (y con ello desaparecerán ciertas garantías)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Diferenciables no-Lineales: *Redes Neuronales*\n",
    "\n",
    "Una solución más general a este tipo de problemas es usar Redes Neuronales (NNs) **como aproximador** de nuestra Función de Valor.\n",
    "\n",
    "De esta forma, ya no necesitamos transformar nuestro espacio de estados, ni conocer cómo funciona el problema, sino que podemos usar **directamente** los datos *crudos* observados (ej: píxeles) como entrada, y obtener como salida de la NN el valor que lo represente.\n",
    "\n",
    "Concretamente, la **Función de Valor serán los pesos (*weights*) de la red neuronal**, que *mapearán* directamente desde una entrada cruda, a algún valor o puntaje que represente ese estado.\n",
    "\n",
    "Las features estarán **implícitas en los pesos** de la red, dejando en manos del **optimizador** de la red neuronal el problema de encontrar buenas features, generalizando a una mayor cantidad de casos, sin necesidad de tener que conocer la naturaleza del problema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conveniente comenzar a pensar en este tipo de diseño de algoritmos de una manera ***más modular***, donde cada módulo puede ser reemplazado por uno equivalente, o que generalice a varios de ellos.\n",
    "\n",
    "En el caso de Q Learning con aproximador lineal, solo basta con reemplazar \"el módulo\" encargado de transformar el espacio mediante las features preprogramadas, por una red neuronal, como se muestra a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pseudocodigo para one step Q-learning con NN \n",
    "(usando funciones de [PyTorch](https://pytorch.org/), como se usará en el resto del TP)\n",
    "\n",
    ">    `Usando NN, calcular Q values para estado observado`\n",
    ">\n",
    ">    `Q(S_t, a) para cada acción a`\n",
    ">\n",
    ">    q = q_net( obs )\n",
    ">    \n",
    ">    `Seleccionar acción de forma epsilon-greedy `\n",
    ">\n",
    ">    action = epsilon_greedy( q )\n",
    ">    \n",
    ">    `Obtenemos el valor de esta acción`\n",
    ">\n",
    ">    qa = q [ action ] \n",
    ">    \n",
    ">    `Realizamos la acción elegida y obtenemos nueva obs`\n",
    ">\n",
    ">    reward, discount, next_obs = env.step( action )\n",
    ">    \n",
    ">    `Usamos NN nuevamente para estimar max valor de prox accion`\n",
    ">\n",
    ">    max_q_next = torch.max( q_net( next_obs ) )\n",
    ">    \n",
    ">    `Calculamos TD-error, con especial atención a NO propagar`\n",
    ">\n",
    ">    `gradiente de next_obs, solo de qa`\n",
    ">\n",
    ">    advantage = reward + discount * max_q_next.detach() - qa\n",
    ">    \n",
    ">    `Definimos loss`\n",
    ">\n",
    ">    `(ver archivo con deducción de por qué norma L2)`\n",
    ">\n",
    ">    q_loss = delta**2 / 2\n",
    ">    \n",
    ">    `Usamos optimizador con respecto a este q_loss`\n",
    ">\n",
    ">    `con Stochastic Gradient Descent, RMSProp, Adam, etc`\n",
    ">\n",
    ">    optimizer.minimize( q_loss )\n",
    "\n",
    "Deducción de por qué normal L2: [./resources/sarsa_updates_are_MSE.ipynb](./resources/sarsa_updates_are_MSE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, *a vanilla implementation* del pseudocódigo de arriba, aunque con pocas garantías de converger, ya que para este simulador particular de Pacman, las observaciones son **caracteres ascii** y no píxeles o algún valor numérico, por lo que hubo que hacer bastante procesamiento de datos, que pudo no haber sido la mejor manera.\n",
    "\n",
    "Se exploraron varias opciones en esta dirección, en especial, para cada caracter ascii diferente observado (10 en total):\n",
    "* un número entre 0 y 1 (0/10, 1/10, 2/10, ... , 9/10)\n",
    "* un one-hot vector de 10 dimensiones\n",
    "* una fusión de las anteriores, aprovechando el triple canal de entrada de colores de la red neuronal convolucional, dividiendo en cada canal:\n",
    " \n",
    " Canal 1: enemigos / agentes\n",
    " \n",
    " Canal 2: objetos interactivos / pellets\n",
    " \n",
    " Canal 3: objetos inertes / muros y límites del mapa\n",
    "\n",
    "A pesar de que conceptualmente parecen ideas sólidas (sobre todo las últimas dos que agregan cierta abstracción/conocimiento previo a la representación del problema), el algoritmo parece no converger, o lo hace muy lentamente.\n",
    "\n",
    "Dada la **amplia** variedad de factores que pueden ser la causa de ésto (hiperparámetros, representación pobre, errores de programación y manejo del modelo, modelo de NN demasiado grande o poco apto, etc) y las no-tan-claras capacidades de convergencia de este tipo de modelo no-lineal, se decidió por avanzar en los temas del trabajo práctico, dejando este algoritmo sin una solución/implementación satisfactoria.\n",
    "\n",
    "Para ver el código del agente '*NNQAgentAllQActionsOut*':\n",
    "[./ cs188x_pacman / qLearningAgents.py](./cs188x_pacman/qLearningAgents.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:10:30.044720Z",
     "start_time": "2019-07-14T02:10:30.040327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't change to ./cs188x_pacman folder. Are you already there?\n",
      "Current directory: /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman\n"
     ]
    }
   ],
   "source": [
    "# Change dir to run python files in subdirectory\n",
    "import os\n",
    "try:\n",
    "    os.chdir('./cs188x_pacman')\n",
    "except:\n",
    "    print(\"Couldn't change to ./cs188x_pacman folder. Are you already there?\")\n",
    "    print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "from pacman import readCommand, runGames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T22:06:21.442651Z",
     "start_time": "2019-07-13T22:06:21.000713Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning 500 episodes of Training\n",
      "Episodes\tMean R lifetime / last 50 ep\tTraining time\n",
      "['West', 'Stop', 'East']\n",
      "tensor([0.2000, 0.1999, 0.2000, 0.2000, 0.2000])\n",
      "['Stop', 'East', 'South']\n",
      "tensor([0.2000, 0.1999, 0.2000, 0.2000, 0.2000])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NNQAgentAllQActionsOut' object has no attribute 'Q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-83ac88f334d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m          '--layout',     'smallGridMoreFood']\n\u001b[1;32m      9\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get game components based on options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrunGames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/pacman.py\u001b[0m in \u001b[0;36mrunGames\u001b[0;34m(layout, pacman, ghosts, display, numGames, record, numTraining, catchExceptions, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m         game = rules.newGame(layout, pacman, ghosts,\n\u001b[1;32m    733\u001b[0m                              gameDisplay, beQuiet, catchExceptions)\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbeQuiet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mgames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/game.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     observation = agent.observationFunction(\n\u001b[0;32m--> 678\u001b[0;31m                         self.state.deepCopy())\n\u001b[0m\u001b[1;32m    679\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/learningAgents.py\u001b[0m in \u001b[0;36mobservationFunction\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastState\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserveTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastAction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/learningAgents.py\u001b[0m in \u001b[0;36mobserveTransition\u001b[0;34m(self, state, action, nextState, deltaReward, terminal_state)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodeRewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdeltaReward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltaReward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstartEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/qlearningAgents.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, state, action, nextState, reward, terminal_state)\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;31m#output = self.getQValues(state, compute_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPolQValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0;31m#print(\"terminal. reward: \", reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/qlearningAgents.py\u001b[0m in \u001b[0;36mgetPolQValue\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# TODO: Find a better way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mpolicy_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0;31m#value=max([self.getQValue(state, a) for a in legalActions])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman/qlearningAgents.py\u001b[0m in \u001b[0;36mgetQValue\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[1;32m    171\u001b[0m         \u001b[0;34m\"*** YOUR CODE HERE ***\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NNQAgentAllQActionsOut' object has no attribute 'Q'"
     ]
    }
   ],
   "source": [
    "# NN Agent-pNNQAgent\n",
    "#options = [\"-pNNQAgentAllQActionsOut\", \"-x500\", \"-n510\", \"-k2\", \"-gDirectionalGhost\",\"-lsmallGridMoreFood\"]\n",
    "prefs = ['--pacman',     'NNQAgentAllQActionsOut',\n",
    "         '--numTraining','500',\n",
    "         '--numGames',   '501',\n",
    "         '--numghosts',  '2',\n",
    "         '--ghosts',     'DirectionalGhost',\n",
    "         '--layout',     'smallGridMoreFood']\n",
    "args = readCommand(prefs)  # Get game components based on options\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensiones\n",
    "\n",
    "El tipo de dificultades anterior no es ajeno al área de Deep Reinforcement Learning en general, por lo que existen varios métodos o *trucos* para mejorar las capacidades de este tipo de algoritmos. Entre ellos:\n",
    "\n",
    "#### DQN con Atari\n",
    "Paper: [Playing Atari with Deep Reinforcement Learning](./resources/Playing_Atari_with_Deep_Reinforcement_Learning_-_DQN.pdf)\n",
    "\n",
    "* **Replay Buffer (DQN):**\n",
    "\n",
    "  Reemplaza el aprendizaje online de los anteriores algoritmos, por sampleo de mini-batches de un buffer/historial de transiciones \n",
    "  $T_{t=0}, T_{t=1}, T_{t=2}, ... , T_{t=n}$\n",
    "  \n",
    "  Siendo $T_{t=i}$: [observación > accion > nueva observación]\n",
    "  \n",
    "  Cada sample se suele observar varias veces antes de ser descartado(4-8 en DQN).\n",
    "  \n",
    "  Si se *samplea* de manera aleatoria entre los datos guardados, obtendremos una data con **características más iid**.\n",
    "  \n",
    "  **OBS: Replay Buffer $\\approx$ Modelo**\n",
    "  \n",
    "  Tener un *replay buffer* suele ser considerado como una forma de *Modelo* (aproximado), ya que las observaciones fueron *samples* del environment y mantienen su distribución.\n",
    "  \n",
    "  Volver a observar datos guardados de este buffer, es equivalente a obtener muestras de un Modelo de tipo *caja negra*, del cual suponemos tiene distribución similar a la del *environment*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Target Network (DQN):**\n",
    "\n",
    "  Se tiene una segunda NN con **el mismo** modelo (de por ejemplo la Value Function) pero con otros parámetros (típicamente llamados $\\theta^{-}$)\n",
    "  \n",
    "  Se usa para realizar *bootstraping*, por lo que la idea detrás es que sea **una función más estable o estacionaria** que la Función de Valor con parámetros $\\theta$.\n",
    "  \n",
    "  Más en la dirección del *Aprendizaje Supervisado*, mientras que con *Replay Buffer* se actualizan los parámetros $\\theta$ en cada mini-batch, los parámetros $\\theta^{-}$ de la *Target Network* se \"actualizan\" de manera más esporádica (ej. cada 10.000 time steps).\n",
    "  \n",
    "  Esta actualización de los parámetros $\\theta^{-}$ es simplemente sobreescribir los $\\theta^{-}$ ya existentes con los $\\theta$ aprendidos usando *Replay Buffer*, y **mantenerlos fijos** durante el resto de la franja de tiempo, en la cual se usa este red para realizar *bootstraping*.\n",
    "  \n",
    "  Cabe destacar que ahora la loss function es función de ambas redes (con $\\theta$ y $\\theta^-$):\n",
    "  \n",
    "  $loss(\\theta) = \\frac{1}{2} \\, (R_{i+1} + \\gamma \\, [\\![ \\mathop{max}_{a} q_{\\theta^-} (S_{i+1}, a) ]\\!] - q_\\theta(S_i, A_i))^2 $\n",
    "  \n",
    "  Donde los dobles corchetes $[\\![ * ]\\!]$ indican que **no** debe *fluir* ese gradiente (en caso contrario es llamada *Residual Network*, generalmente con **menor performance**, por no respetar *causalidad*).\n",
    "  \n",
    "  Con esta *Target Network* extra se logra (no siempre) mayor estabilidad en el algoritmo, aunque **no queda claro cuándo y en qué tipo de problemas**.\n",
    "  \n",
    "  Una observación de [Vlad Mnih](https://www.cs.toronto.edu/~vmnih/) (uno de los autores de DQN) es que ***a medida que la capacidad o \"tamaño\" del modelo de red neuronal incrementa, la Target Network se vuelve menos importante***.\n",
    "  \n",
    "  Una hipótesis (no formal) de por qué esto es así, se basa en que teniendo un **modelo neuronal pequeño**, los updates de cada acción afectarán **gran parte de la red**, volviendo a nuestro objetivo **altamente no-estacionario**.\n",
    "  \n",
    "  Mientras que con un modelo neuronal de **mayor capacidad**, cada update afectará a una **pequeña porción de la red**, por lo que el objetivo no cambiará tanto, **ganando estabilidad** y restandole importancia a los beneficios que se puedan ganar al incluir una  Target Network.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de DQN con Atari, también fueron usadas otras estrategias como **pre-procesamiento** de los datos de entrada, ***hyper parameter tunning*** corriendo múltiples instancias con un gran poder de hardware para elegir los mejores parámetros, como con el discount factor de 0.99 elegido sobre varios, para tener cierto horizonte sobre el cual optimizar.\n",
    "\n",
    "Se logró cierta generalidad en el resultado, ya que **una vez hayados estos valores, se fijaron para todos los juegos testeados**, pero al mismo tiempo da una idea de que sigue siendo necesaria (al menos para DQN) cierta *fuerza bruta* que suplante la falta de conocimiento en el tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-step Return\n",
    "\n",
    "![n-step-td](./img/n-step-td.jpg)\n",
    "\n",
    "Otro método que suele ayudar a encontrar un equilibrio entre **bias** y **varianza** para estos modelos, es usar como *Return* $G_t$ algo intermedio entre TD return ($G_t^{(1)}$) y MC return ($G_t^{(\\infty)}$): \n",
    "\n",
    "##### n-step TD return\n",
    "\n",
    "Denotaremos con superíndice $^{(n)}$ al return correspondiente a $n$ pasos (acciones por *time-step*) que tomaremos con rewards $R_{t+1}, R_{t+2}, ... ,R_{t+n}$, y una estimación a futuro $\\tilde{V}(S_{t+n})$ del último estado  (*bootstrap*), cada uno con sus respectivos descuentos $\\gamma^{i-1}$:\n",
    "\n",
    "$$G_t^{(n)} = R_{t+1} + \\gamma\\, R_{t+2}\\, + \\gamma^2\\, R_{t+3} + \\, ... \\, + \\, \\gamma^{n-1} R_{t+n} + \\gamma^{n}\\, \\tilde{V}(S_{t+n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t^{(n)} = ( \\sum_{i=1}^n \\gamma^{i-1}\\, R_{t+i} ) + \\gamma^n \\, \\tilde{V}(S_{t+n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que obtenemos ***n-step Temporal-Difference Learning***\n",
    "\n",
    "con update:\n",
    "\n",
    "$\\tilde{V}(S_t) \\leftarrow \\tilde{V}(S_t) + \\alpha \\, (G_t^{(n)} - \\tilde{V}(S_t))$\n",
    "\n",
    "lo cual nos permite tener un control sobre un rango de acción $n$, que será otro hyperparámetro cuyo valor dependerá de cada problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener una intuición de cómo mejora nuestro algoritmo, consideremos el siguiente ejemplo:\n",
    "\n",
    "> Con política de ***random walk*** y\n",
    "> un **único reward** en un extremo del MDP\n",
    ">    \n",
    "> buscamos mejorar nuestra **estimación de la Value Function** (el valor de cada estado A-E) usando TD-error con n-step return:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerar para 19 estados (A a S), por simplicidad solo se muestran 5  (A a E):\n",
    "![n-step-td-random-walk-silver-example](./img/n-step-td-random-walk-silver-example.png)\n",
    "\n",
    "\n",
    "![n-step-td-random-walk-hado-example](./img/n-step-td-random-walk-hado-example.png)\n",
    "\n",
    "En el eje vertical tenemos el **error de predicción**\n",
    "\n",
    "En el eje horizontal **step size** (o *learning rate*) $\\alpha$\n",
    "\n",
    "Podemos ver cómo con $G_t^{(1)}$ (o TD(0)) la curva de la **Raiz del Error Cuadrático Medio (RMSE)** se mantiene alta a pesar de lo distintos valores que puede tomar $\\alpha$.\n",
    "\n",
    "Mientras que con returns $G_t^{(n)}$ con $n$ entre 2 y 8 para el caso on-line, ó 4 y 16 para el offline, obtenemos el menor error **si elegimos el $\\alpha$ adecuado**.\n",
    "\n",
    "Y para $n > 16$ el error sigue creciendo a medida que nos acercamos a Monte Carlo ($G_t^{(\\infty)}$)\n",
    "\n",
    "Este error se verá directamente reflejado en las decisiones y consecuentes observaciones que tome el agente.\n",
    "\n",
    "* Si estamos en el rango de $n$ y $\\alpha$ de menor error, estaremos yendo en la dirección adecuada, por lo que \"avanzaremos\" hacia nuestro objetivo a mayor velocidad.\n",
    "\n",
    "* De manera opuesta, a mayor RMSE, el aprendizaje será más lento, jamás convergeremos a óptimo cercano al buscado, ya que el objetivo se seguirá \"moviendo\" a causa de la varianza, o \"apuntaremos\" en la dirección equivocada a causa del bias, convergiendo a un óptimo local.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T03:59:00.800520Z",
     "start_time": "2019-07-08T03:59:00.591805Z"
    }
   },
   "source": [
    "---\n",
    ">Fuente de gran parte de la sección de aproximación:\n",
    ">\n",
    ">*ReinforcementLearning 5: Function Approximation and Deep Reinforcement Learning*\n",
    ">\n",
    ">Hado van Hasselt - DeepMind\n",
    ">\n",
    ">https://www.youtube.com/watch?v=wAk1lxmiW4c\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Approximation\n",
    "\n",
    "Hasta ahora, los algoritmos vistos hicieron uso directo de la ***Función* de Valor*** para determinar su política óptima:\n",
    "\n",
    "1. Elegir la **acción con máximo valor** a partir del estado observado.\n",
    "   \n",
    "2. Para agregar **exploración**, se elige una acción **al azar** con probabilidad epsilon $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se podría entonces considerar que **aprender la Función de Valor** es un problema **más general** que el que buscamos resolver.\n",
    "\n",
    "Esto es porque **aprender la Función de Valor** a partir de una observación para **luego elegir la acción con máximo valor** entre las opciones posibles, podría reducirse a **aprender directamente la acción a elegir, a partir de una observación**, sin pasar por una función de valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *When solving a problem of interest, do not solve a more general problem as an intermediate step.*\n",
    "> \n",
    "> *Try to get the answer that you really need but not a more general one.*\n",
    "\n",
    "$$- \\text{Vladimir Vapnik, on 'transduction'}$$\n",
    "http://www.cs.bham.ac.uk/~jdk/transductive.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensando en eficiencia de datos, si lo que buscamos es un comportamiento óptimo, al resolver un problema más general se necesitará **al menos** la misma cantidad de data.\n",
    "\n",
    "Por lo que aprender la política **directamente** reduciría este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Consideramos nuevamente los tres grandes grupos de aprendizaje por refuerzo.\n",
    "\n",
    "* **Model-based RL**\n",
    "\n",
    " `+` Se aprende usando Aprendizaje Supervisado (well understood)\n",
    " \n",
    " `+` Representa **toda** la información de la dinámica del *environment*\n",
    " \n",
    " `-` Muy costoso de aprender, sobre todo para problemas complejos\n",
    " \n",
    " `-` Ineficiente con los datos; aprende dinámica de **todo** lo observado\n",
    " \n",
    " `-` Objetivo captura información irrelevante\n",
    " \n",
    " \n",
    "* **Value-based RL**\n",
    "\n",
    " `+` Objetivo cercano a Control óptimo\n",
    " \n",
    " `+` *Well-understood*\n",
    " \n",
    " `-` Puede ser muy costoso de aprender, aunque la política óptima sea simple\n",
    " \n",
    " `-` Objetivo captura información irrelevante\n",
    " \n",
    "\n",
    "* **Policy-based RL**\n",
    " \n",
    " `+` Objetivo correcto: *Encontrar política óptima para estados observados*\n",
    " \n",
    " `-` Ignora información que puede ser relevante. Ineficiente con los datos.\n",
    " \n",
    " `-` Aprendizaje muy lento al comienzo, sin garantías de aprender algo *útil*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic\n",
    "\n",
    "![value-policy-actor-critic](./img/value-policy-actor-critic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Una forma de mantener el objetivo de ***policy-based RL*** sin perder eficiencia en los datos, es **aprender tanto la Value Function como la Policy Function**.\n",
    "\n",
    "Mientras que el objetivo sigue siendo **aprender la política óptima**, se puede aprovechar gran parte del modelo de red neuronal **para también**, tener una representación de los **valores de cada estado**.\n",
    "\n",
    "De esta forma no solo **el agente** tiene una representación más completa del estado observado, sino que **el gradiente** a seguir contendrá **información más relevante** sobre un posible mejor resultado, que si solo representara información de la política.\n",
    "\n",
    "Este tipo de modelos con *Función de Valor* y *Función de Política* se llaman ***Actor-Critic***, donde ***\"el Crítico\"*** es la **Función de Valor**, y ***\"el Actor\"*** es la **Función de Política** a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![actor-critic-model](./img/actor-critic-model.png)\n",
    "\n",
    "El digrama representa un modelo básico de Actor-Critic, donde todos los parámetros están compartidos, menos en la últiima capa (*layer*) donde se dividen en los valores para cada estado posible (Critic) o la política a seguir (Actor) con probabilidades para cada acción.\n",
    "\n",
    "$\\pi (a_t | s_t; \\theta)$ y $V (s_t ; \\theta_v)$ son las funciones correspondientes a cada objetivo (Value/Policy), con $\\theta$ y $\\theta_v$ todos los parámetros de cada una (compartidos e individuales).\n",
    "\n",
    "$\\pi (a_t | s_t; \\theta)$: Devuelve la probabilidad de elegir cada acción. Las $n$ salidas suman 1.\n",
    "\n",
    "$V (s_t ; \\theta_v)$: Devuelve un valor que representa el puntaje del estado presente, siguiendo la política $\\pi_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ventajas\n",
    "\n",
    " * Buenas propiedades de convergencia\n",
    " \n",
    " \n",
    " * Ganamos características de *Policy models*:\n",
    " \n",
    "   1. Facilmente se puede extender a **acciones de espacio contínuo**\n",
    "   \n",
    "      * Se cambia cantidad de outputs por solo dos valores: **Media** y **Desviación estándar** de una **Distribución Normal**\n",
    "      \n",
    "   2. Puede aprender **políticas estocásticas**: control sobre las **probabilidades** de cada acción \n",
    "      * Puede ser usado como de **qué tanta confianza** se tiene en cierta acción\n",
    "      * El agente puede controlar **qué tanto explora**\n",
    "      * Puede resolver problemas con **información oculta** (*Partially Observable MDPs* o *POMDPs*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Gradient\n",
    "\n",
    "Necesitamos **cuantificar la calidad de la policy $\\pi_\\theta$** para poder mejorarla.\n",
    "\n",
    "Para eso es necesario convertiro en un **problema de optimización**.\n",
    "\n",
    "#### Policy Objective Functions ( funciones objetivo para las políticas)\n",
    "\n",
    "Vamos a querer **maximizar** estos objetivos, que representarán cierto *puntaje* de la política:\n",
    "\n",
    "1. ***Start value*** $J_1(\\theta)$: Si el *environment* es episódico, puede usarse ***start value*** $J_1(\\theta)$ (valor de comienzo)\n",
    " \n",
    " $$J_1(\\theta) = \\mathbf{v}_{\\pi_\\theta} (s_1)$$\n",
    " \n",
    " A partir de un estado inicial $(s_1)$, queremos maximizar $\\mathbf{v}_{\\pi_\\theta}(s_1)$, que es el valor del primer estado, siguiendo la política $\\pi_\\theta$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. Si el *environment* no tiene un *start state* bien definido, tenemos dos opciones similares entre sí:\n",
    " \n",
    " * ***Average value*** $J_{avV} (\\theta)$ (valor promedio)\n",
    " \n",
    "  $$J_{avV}(\\theta) = \\sum_s d_{\\pi_\\theta} (s) \\, \\mathbf{v}_{\\pi_\\theta} (s)$$\n",
    "  \n",
    "  $d_{\\pi_\\theta}(s)$ distribución **inducida por la política $\\pi_\\theta$** que el agente sigue.\n",
    "  \n",
    "  con $d_{\\pi_\\theta}(s) = p \\, (S_t=s | \\pi_\\theta)$ la probabilidad de estar en el estado $s$ a largo plazo\n",
    "\n",
    "  `+` Equivalente al *ratio* de tiempo que estaremos en $s$ **siguiendo la política $\\pi$**\n",
    " \n",
    " `+` Para **estados continuos** la integral reemplaza la sumatoria.\n",
    " \n",
    " `+` Da **prioridad a los estados que visita**, sin darle importancia a los que no.\n",
    " \n",
    " ---\n",
    " \n",
    "3. ***Average reward per time step*** $J_{avR} (\\theta)$ (reward promedio en cada paso)\n",
    " \n",
    " $$J_{avR}(\\theta) = \\sum_s d_{\\pi_\\theta} (s) \\, \\sum_a \\pi_\\theta(s,a) \\sum_r p(r|s,a)\\, r$$\n",
    " \n",
    " `+` Notar que **solo usa *rewards* inmediatos**, no $\\mathbf{v}_{\\pi_\\theta}(s)$\n",
    " \n",
    " Ésto vale pues por la sumatoria de afuera, estamos sumando sobre todos los estados, que capturan la dinámica de la misma forma que $\\mathbf{v}_{\\pi_\\theta}(s)$\n",
    "\n",
    " `+` Muy común su uso en plots o gráficos en investigación y papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:**\n",
    "\n",
    "Es importante remarcar que las distribuciones $d_{\\pi_\\theta}$ de los estados son **en función de $\\pi_\\theta$**, la política paramétrica actual.\n",
    "\n",
    "Esto nos asegura que en el caso de tomar samples, sean sobre la distribución correcta, que es siguiendo la politica política actual, que es a quien queremos corregir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to sample\n",
    "\n",
    "#### Toy example\n",
    "\n",
    "Consideramos $J(\\theta) = \\mathbf{E} \\, [ R(S,A) ]$ para el caso ***bandits***\n",
    "\n",
    "con $\\mathbf{E} \\, [ R(S,A) ]$ esperanza **sobre estados y acciones**\n",
    "\n",
    "Queremos **un gradiente**, pero samplear esa esperanza nos devuelve **un valor escalar** $R_{t+1}$ que no podemos usar para actualizar la función de política.\n",
    "\n",
    "Para solucionar eso, podemos usar esta equivalencia (demostración a continuación):\n",
    "\n",
    "> $$\\nabla_\\theta \\mathbf{E} \\, [ R(S,A) ] =  \\mathbf{E} \\, [ \\nabla_\\theta \\log \\pi_\\theta (A | S) \\, R(S,A) ]$$\n",
    "\n",
    "En la que ahora **tenemos la esperanza de un gradiente** que sí podemos samplear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Score function trick\" o \"REINFORCE trick\" o \"log-likelihood trick\"\n",
    "\n",
    "sea:\n",
    "\n",
    "$d(s)$ una **distribución sobre los estados** (variables aleatorias)\n",
    "\n",
    "$\\pi_\\theta (a | s)$ **distribución sobre las políticas** (estocásticas) \n",
    "\n",
    "$R (s, a)$ el reward (o reward esperado) luego de tomar acción $a$ en estado $s$ \n",
    "\n",
    "$$\\nabla_\\theta \\mathbf{E} \\, [ R(S,A) ] = \\nabla_\\theta \\sum_s d(s) \\sum_a \\pi_\\theta (a | s) \\, R (s, a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demostración\n",
    "\n",
    "Movemos el gradiente al interior de la sumatoria, ya que solo $\\pi_\\theta$ depende de $\\theta$\n",
    "\n",
    "$$= \\sum_s d(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) \\, R(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicamos y dividimos por $\\pi_\\theta (a|s)$, para darle forma de esperanza (cada acción es pesada por la probabilidad de ser elegida)\n",
    "\n",
    "$$= \\sum_s d(s) \\sum_a \\pi_\\theta (a|s) \\frac{\\nabla_\\theta \\pi_\\theta (a|s)}{\\pi_\\theta(a|s)} R(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabiendo $\\nabla \\log y = \\frac{1}{y} \\nabla y$, reescribimos\n",
    "$$= \\sum_s d(s) \\sum_a \\pi_\\theta (a|s) \\, \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, R(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo cual es la esperanza del gradiente del logaritmo de la política $\\pi$, multiplicado por el reward $R(s,a)$\n",
    "\n",
    "> $$\\nabla_\\theta \\mathbf{E} \\, [ R(S,A) ] = \\mathbf{E} \\, [ \\nabla_\\theta \\log \\pi_\\theta (A | S) \\, R(S,A) ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ahora podemos samplear y usar como update para los parámetros $\\theta$\n",
    "\n",
    "> $$\\theta_{t+1} = \\theta_t + \\alpha \\, R_{t+1} \\, \\nabla_\\theta \\log \\pi_{\\theta_t} (A_t | S_t)$$\n",
    "\n",
    "Que bajo condiciones normales **podemos asumir que alcanzará un óptimo local**, ya que sigue el gradiente de nuestro objetivo:\n",
    "\n",
    "> la **Función de Política paramétrica $\\pi_\\theta$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La intuición detrás de este update es que la política será modificada de forma tal que **las acciones con mayor reward aumentarán su probabilidad de ser elegidas**, mientras que **las de menor reward o reward negativo, incrementarán menos o reducirán la probabilidad de ser elegidas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo concreto de update con política *Softmax* (se usa a menudo)\n",
    "\n",
    "Sea $\\pi_\\theta(a|s)$ una política softmax sobre una función de preferencias $h(s,a)$\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\frac{e^{h(s,a)}}{\\sum_b e^{h(s,b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente de la probabilidad logarítmica $\\log \\pi_\\theta (a|s)$ será\n",
    "\n",
    "> $$ \\nabla_\\theta \\log \\pi_{\\theta}(a|s) = \\nabla_\\theta h(s,a) - \\sum_b \\pi_\\theta(b|s) \\nabla_\\theta h(s,b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queremos extenderlo al caso multi-step, donde\n",
    "\n",
    "1. Hay **secuencialidad**\n",
    "2. No solo hay rewards inmediatos, sino *valores* (*values*) con dependencias temporales\n",
    "\n",
    "Para ello tenemos las siguientes **herramientas teóricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem (con demostración)\n",
    "\n",
    "* Nos permite *reemplazar* los rewards instantáneos en la ecuación antes derivada, con *long-term values* $q_\\pi(s,a)$\n",
    "\n",
    "* Aplica para objetivos (*objectives*):\n",
    "  * Estado de comienzo $J=J_1$ (start state objective)\n",
    "  * Reward promedio $J_{avR}$ (average reward objective)\n",
    "  * Valor promedio $\\frac{1}{1-\\gamma} J_{avV}$ (average value objective)\n",
    "\n",
    "#### Teorema:\n",
    "\n",
    "> Para cualquier política diferenciable $\\pi_\\theta(s,a)$,\n",
    ">\n",
    "> para cualquier función objetivo (de la política) $J=J_1$, $J_{avR}$, ó $\\frac{1}{1-\\gamma} J_{avV}$,\n",
    ">\n",
    "> el gradiente de la política es\n",
    "$$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ q_{\\pi_\\theta}(S,A) \\, \\nabla_\\theta \\log \\pi_\\theta (A|S) \\right]$$\n",
    "\n",
    "Con la esperanza $\\mathbf{E} [ * ]$ tomada sobre estados y acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient sobre trayectorias\n",
    "\n",
    "La siguiente es una demostración de por que no es necesario saber cómo la política afecta los distintos estados.\n",
    "\n",
    "Sea la trayectoria de datos *zeta* $\\zeta = S_0,A_0,R_1,S_1,A_1,R_2,S_2,...$ con return $G(\\zeta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que el return $G(\\zeta)$ es una muestra insesgada del Valor de $\\zeta$ \n",
    "\n",
    "$$\\nabla_\\theta J_\\theta (\\pi) = \\nabla_\\theta \\mathbf{E}[G(\\zeta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre lo cual podemos aplicar el score / REINFORCE / log-likelihood trick\n",
    "\n",
    "$$\\nabla_\\theta \\mathbf{E}[G(\\zeta)] = \\mathbf{E}[G(\\zeta) \\nabla_\\theta \\log p(\\zeta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expandimos $\\nabla_\\theta \\log p(\\zeta)$, que es la probabilidad de obtener esta trayectoria en particular\n",
    "\n",
    "$$\\nabla_\\theta \\log p(\\zeta) = \\nabla_\\theta \\log \\, \\left[ \\, p(S_0) \\pi(A_0|S_0) p(S_1|S_0, A_0) \\pi(A_1|S_1) \\, ...\\, \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logaritmo del producto es suma de los logaritmos\n",
    "\n",
    "$$= \\nabla_\\theta  \\left[ \\, \\log p(S_0) + \\log \\pi(A_0|S_0) + \\log  p(S_1|S_0, A_0) + \\log \\pi(A_1|S_1) \\, + \\, ...\\, \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar la derivada con respecto a $\\theta$ (parámetros de la política), solo quedan los términos que la involucran\n",
    "\n",
    "$$= \\nabla_\\theta  \\left[ \\, \\log \\pi_\\theta(A_0|S_0) + \\log \\pi_\\theta(A_1|S_1) \\, + \\, ...\\, \\right]$$\n",
    "\n",
    "$$\\nabla_\\theta \\log p(\\zeta) = \\nabla_\\theta \\sum_{t=0} \\log \\pi_\\theta (A_t|S_t) $$\n",
    "\n",
    "Lo que indica que **los parámetros solo afectan las decisiones que tome el agente** (al seguir esa política) y no la dinámica del environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo\n",
    "$$\\nabla_\\theta J_\\theta (\\pi) = \\mathbf{E}[G(\\zeta) \\nabla_\\theta \\log p(\\zeta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =  \\mathbf{E}\\left[ G(\\zeta) \\, \\nabla_\\theta \\sum_{t=0} \\log \\pi (A_t|S_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expandiendo el return $G(\\zeta)$ como la suma de los rewards\n",
    "\n",
    "> $$\\nabla_\\theta J_\\theta (\\pi) = \\mathbf{E}\\left[\\left( \\sum_{t=0} R_{t+1} \\right)\\left( \\nabla_\\theta \\sum_{t=0} \\log \\pi (A_t|S_t) \\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más general, el caso con discount factor $\\gamma$\n",
    "\n",
    "> $$\\nabla_\\theta J_\\theta (\\pi) = \\mathbf{E}\\left[\\left( \\sum_{t=0} \\gamma^t R_{t+1} \\right)\\left( \\nabla_\\theta \\sum_{t=0} \\log \\pi (A_t|S_t) \\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baselines\n",
    "\n",
    "Una forma de **reducir la varianza** de este tipo de objetivos es **usar la diferencia** entre el **reward** y una ***baseline*** en lugar de solamente el return anteriormente usado.\n",
    "\n",
    "De esta forma no haremos *updates* proporcionales al reward/return directamente, sino a la **diferencia** de este reward con respecto a la *baseline*, la cual es basicamente un **reward promedio esperado**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, sea $b$ baseline, función que no dependa de las acciones (aunque puede depender de los estados)\n",
    "\n",
    "Expandimos $A_t$ de modo que ahora las acciones no son aleatorias, solo los estados\n",
    "$$\\mathbf{E} \\left[ b \\nabla_\\theta \\log \\pi(A_t|S_t) \\right] = \\mathbf{E} \\left[ \\sum_a \\pi(a|S_t) \\, b \\, \\nabla_\\theta \\log \\pi(a|S_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, score function trick, pero en reversa\n",
    "\n",
    "$$= \\mathbf{E} \\left[  \\, b \\, \\nabla_\\theta \\sum_a \\pi(a|S_t) \\right]$$\n",
    "\n",
    "$$= \\mathbf{E} \\left[  \\, b \\, \\nabla_\\theta \\, 1 \\right]$$\n",
    "\n",
    "$$ = 0$$\n",
    "\n",
    "Esto es posible pues $b$ **no** depende de las acciones, por lo que podemos *moverlo* fuera de la sumatoria.\n",
    "\n",
    "Si dependiera, ésto **no** sería posible.\n",
    "\n",
    "Lo que lleva a dudar de la utilidad de esta *baseline*, pues sabemos que **los rewards dependen de las acciones tomadas**.\n",
    "\n",
    "Pero por **causalidad**, no dependen **de las que se tomarán**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a la demostracion anterior, sabíamos que\n",
    "\n",
    "> $$\\nabla_\\theta J_\\theta (\\pi) = \\mathbf{E}\\left[\\left( \\sum_{t=0} R_{t+1} \\right)\\left( \\nabla_\\theta \\sum_{t=0} \\log \\pi (A_t|S_t) \\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularmente $\\sum_{t=0}^k R_{t+1}$ **no** depende de las acciones en tiempos **posteriores** $A_{k+1}, A_{k+2}, A_{k+3} ...$\n",
    "\n",
    "Reordeno sumatorias\n",
    "$$= \\mathbf{E}\\left[ \\sum_{t=0} \\nabla_\\theta \\log \\pi (A_t|S_t) \\sum_{i=0} R_{i+1} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo considero rewards $R_{i+1}$ **posteriores** a acciones $A_{t}$\n",
    "$$= \\mathbf{E}\\left[ \\sum_{t=0} \\nabla_\\theta \\log \\pi (A_t|S_t) \\sum_{i=t} R_{i+1} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribo la última sumatoria como el valor $q$ de la política $\\pi$, pues está dentro de la esperanza\n",
    "> $$= \\mathbf{E}\\left[ \\sum_{t=0} \\nabla_\\theta \\log \\pi (A_t|S_t) q_\\pi(S_t,A_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es exactamente el resultado de **Policy Gradient Theorem** (excepto por un factor de escalamiento en cuanto a cantidad de estados considerados)\n",
    "\n",
    "> $$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ q_{\\pi_\\theta}(S,A) \\, \\nabla_\\theta \\log \\pi_\\theta (A|S) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de los updates será ahora usando $\\nabla_\\theta \\log \\pi (A_t|S_t)$ de esta acción $A_t$ particular, multiplicado por el valor estimado que tendrá esta acción.\n",
    "\n",
    "De esta forma **podemos obtener muestras individuales** o por ***batches*** sin necesidad de completar la suma para todo $t$, y aún así tener un **gradiente válido** para utilizar en el algoritmo de actualización política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baselines, again\n",
    "\n",
    "Como fue mencionado anteriormente, usar *baselines* reduce la varianza de nuestro modelo.\n",
    "\n",
    "Por lo que podemos aplicarla (nuevamente) a nuestro objetivo, el resultado final del ***Policy Gradient Theorem***\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (A_t|S_t) \\,  q_{\\pi_\\theta}(S_t,A_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una buena *baseline* es $\\mathbf{v}_{\\pi}(S_t)$, ya que no depende de las acciones y podemos restarla al valor de la acción\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (A_t|S_t) \\, \\left( q_{\\pi_\\theta}(S_t,A_t) - \\mathbf{v}_{\\pi_\\theta} (S_t) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde típicamente usamos **aproximaciones** y no los valores reales:\n",
    "\n",
    "* $\\mathbf{v}_w(s)$ se estima explícitamente (ej con on policy TD learning)\n",
    "\n",
    "\n",
    "* $q_{\\pi_\\theta}(S_t,A_t)$ se obtiene como una muestra $G_t^{(n)}$\n",
    "\n",
    "  pues\n",
    "  \n",
    "  $q_{\\pi_\\theta}(S_t,A_t) \\approx G_t^{(n)}$\n",
    "  \n",
    "   .\n",
    "  * Ejemplo de muestra (*sample*) con **1-step** return, para usar en lugar de $q_{\\pi_\\theta}(S_t,A_t)$:\n",
    "  \n",
    "    $q_{\\pi_\\theta}(S_t,A_t) \\approx G_t^{(1)} = R_{t+1} + \\gamma \\, \\mathbf{v}_w(S_{t+1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas aproximaciones juntas son exactamente el **1-step TD-error** o ***advantage***\n",
    "\n",
    "> $$R_{t+1} + \\gamma \\, \\mathbf{v}_w(S_{t+1}) - \\mathbf{v}_w(S_t)$$\n",
    "\n",
    "Reemplazando una vez más en el resultado del Policy Gradient Theorem con baseline:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (A_t|S_t) \\, \\left( q_{\\pi_\\theta}(S_t,A_t) - \\mathbf{v}_{\\pi_\\theta} (S_t) \\right) \\right]$$\n",
    "\n",
    "> $$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (A_t|S_t) \\, \\left( R_{t+1} + \\gamma \\, \\mathbf{v}_w(S_{t+1}) - \\mathbf{v}_w(S_t) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terreno conocido\n",
    "\n",
    "El *critic* (la *value function*) está resolviendo ***Policy Evaluation*** para la política actual.\n",
    "\n",
    "Quiere conocer *el valor* de la política $\\pi_\\theta$ con parámetros actuales $\\theta$\n",
    "\n",
    "Soluciones a esta pregunta fueron exploradas en TPs anteriores:\n",
    "* Monte-Carlo policy evaluation\n",
    "* Temporal-Difference learning\n",
    "* n-step TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic: Pseudocódigo e Implementación\n",
    "\n",
    "Haciendo uso de las herramientas matemáticas anteriores, podemos formular en pseudocódigo un agente de tipo ***Actor-Critic***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Actor-Critic\n",
    "\n",
    "* **Critic:** Actualiza los parámetros $w$ de $\\mathbf{v}_w$ usando ***1-step TD*** (podría ser n-step)\n",
    "* **Actor:** Actualiza los parámetros $\\theta$ de $\\pi_\\theta$ usando ***Policy Gradient***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function** ADVANTAGE_ACTOR_CRITIC\n",
    "> `# Inicializar estado inicial y weights de ambas redes`\n",
    "> \n",
    "> Initialize $s$, $w$, $\\theta$\n",
    ">  \n",
    ">  **for** $t = 0,1,2,...$ **do**\n",
    ">\n",
    ">>    `# Elegir una accion a partir de politica pi`\n",
    ">>\n",
    ">>    Sample $A_t \\sim \\pi_\\theta(S_t)$\n",
    ">>\n",
    ">>    `# Tomar esa accion > recibir nueva observacion`\n",
    ">>\n",
    ">>    Sample $R_{t+1}$ and $S_{t+1}$\n",
    ">>\n",
    ">>    `# 1-step TD-error o Advantage function`\n",
    ">>\n",
    ">>    $\\delta_t = R_{t+1} + \\gamma \\, \\mathbf{v}_w(S_{t+1}) - \\mathbf{v}_w(S_t)$\n",
    ">>\n",
    ">>    `# Update a parametros del Critic con TD(0)`\n",
    ">>\n",
    ">>    $w \\leftarrow w + \\beta \\, \\delta_t \\nabla_w \\mathbf{v}_w(S_t)$\n",
    ">>\n",
    ">>    `# Update a parametros del Actor con Policy Gradient`\n",
    ">>\n",
    ">>    $\\theta \\leftarrow \\theta + \\alpha \\, \\delta_t \\, \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)$\n",
    ">\n",
    ">  **end for**\n",
    ">\n",
    ">  `# Update asincrónico a parámetros globales usando gradientes locales`\n",
    "\n",
    "**end function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo $\\alpha , \\beta$ step-sizes, denotados con símbolos diferentes para indicar que no necesariamente deben ser iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más formalmente, del paper: [***\"Asynchronous Methods for Deep Reinforcement Learning\"***](./resources/Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning.pdf)\n",
    "\n",
    "![./img/A3C-pseudocode.png](./img/A3C-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic: Detalles de implementación\n",
    "\n",
    "En la práctica, Actor-Critic suele ser aplicado junto con otras caractéristicas que lo vuelven más **robusto y eficiente** con los datos, al que suele llamarse ****Advantage Actor-Critic***.\n",
    "\n",
    "* Es posible agregar una **representación** no solo del estado observado, sino algo con información del pasado.\n",
    "  \n",
    "  ej: Con una capa de ***LSTM***: agrega la representación\n",
    "  \n",
    "  $( S_{t-1}, O_t) \\rightarrow S_t$\n",
    "  \n",
    "  \n",
    "* Se mantienen las dos redes (funciones paramétricas) Value y Policy Functions:\n",
    "  \n",
    "  * $\\mathbf{v}_w: S \\rightarrow \\mathbf{v}$\n",
    "  \n",
    "  * $\\pi_\\theta: S \\rightarrow \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Asynchronous Actor-Critic**\n",
    "\n",
    "  Usa **copias o variantes de la política** y las corre en varias instancias del simulador (ej Atari) con políticas $\\pi_\\theta^m$, donde $m$ es el índice que caracteriza a esta variante de política en particular, corriendo cada una en un proceso separado.\n",
    "  \n",
    "  $( S_{t-1}^m,O_t^m) \\rightarrow S_t$ para cada $m$ proceso\n",
    "  \n",
    "  De esta manera se tienen **más datos al mismo tiempo**, similares entre sí, pero que permiten capturar **más información** del ambiente.\n",
    "  \n",
    "  Policy gradient se aplica de la misma manera que antes, pero **el gradiente de cada proceso** se usa para actualizar **los pesos de un modelo central**.\n",
    "  \n",
    "  Usualmente se mantienen las políticas para todos los procesos, o en el caso contínuo, se samplean las acciones usando distintas distribuciones.\n",
    "  \n",
    "  Sigue manteniendo **una sola red para el Crítico** (Value Function).\n",
    "  \n",
    "  Recibe el nombre al utilizarlo junto con un grupo de ***parámetros central*** $\\theta^+$ al que cada ciertas iteraciones, cada una de estas copias corriendo por separado actualiza de manera asincrónica.\n",
    "  \n",
    "  Los modelos locales copian los pesos del modelo global **luego de cada episodio**, manteniendo una política actualizada.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sigue haciendo uso de **$n$-step TD loss** para $\\mathbf{v}_w$\n",
    "  \n",
    "  $$l(w) = \\frac{1}{2} \\left( G_t^{(n)} - \\mathbf{v}_w(S_t) \\right)^2 $$\n",
    "  \n",
    "  donde $G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} \\, + \\, ... + \\, \\gamma^{n-1} [\\![ \\mathbf{v}_w(S_{t+n})]\\!]$\n",
    "  \n",
    "  con $[\\![ * ]\\!]$ indicando que debe detenerse el gradiente al usar bootstraping evaluando el ultimo valor en la NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se utiliza **una variante** de **n-step REINFORCE** ***loss*** para actualizar $\\pi_\\theta$\n",
    "  \n",
    "  Lenguajes como Tensorflow, Pytorch, etc, utilizan optimizadores que esperan cierto ***loss*** a minimizar\n",
    "  \n",
    "  En las demostraciones de arriba con *Policy Gradient*, obtuvimos un **gradiente**, un *update*:\n",
    "  \n",
    ">  $$\\nabla_\\theta J(\\theta) =  \\mathbf{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (A_t|S_t) \\, \\left( G_t^{(n)} - \\mathbf{v}_w(S_t) \\right) \\right]$$\n",
    "\n",
    "  En forma de update, lo escribimos como:\n",
    ">  $$\\theta \\leftarrow \\theta + \\alpha \\, \\delta_t \\, \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por lo que debemos reescribir nuestro objetivo para que cuando el optimizador tome su gradiente, recupere (al derivar) el update que queremos\n",
    "\n",
    "> **Loss**\n",
    "> $$l(\\theta) = \\left[ G_t^{(n)} - \\mathbf{v}_w(S_t) \\right] \\log \\pi_\\theta (A_t, S_t)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple demostración:**\n",
    "\n",
    "$\\left[ G_t^{(n)} - \\mathbf{v}_w(S_t) \\right]$ no depende de $\\theta$\n",
    "\n",
    "Derivamos\n",
    "$$l'(\\theta) = \\left[ G_t^{(n)} - \\mathbf{v}_w(S_t) \\right] \\frac{1}{\\pi_\\theta(A_t, S_t)} \\nabla_\\theta \\pi_\\theta(A_t, S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mismo truco que antes: $\\nabla \\log y = \\frac{1}{y} \\nabla y$\n",
    "\n",
    "$$l'(\\theta) = \\left[ G_t^{(n)} - \\mathbf{v}_w(S_t) \\right] \\nabla \\log \\pi_\\theta(A_t, S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* uso de **Optimizadores** para minimizar ***losses***\n",
    "\n",
    "  Teniendo ahora los *updates* en forma de *losses* para ambas funciones de Valor y Política, se pueden usar directamente los optimizadores de la librería/lenguaje usado (ej Adam optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> Fuente de gran parte de la sección de *Policy Gradient*:\n",
    ">\n",
    ">Reinforcement Learning 6: Policy Gradients and Actor Critics\n",
    ">\n",
    ">https://www.youtube.com/watch?v=bRfUxQs6xIM\n",
    ">\n",
    ">Hado van Hasselt - DeepMind\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor Critic (A3C)\n",
    "\n",
    "Con los detalles de implementación mencionados, ya puede programarse y esperar buenos resultados usando valores estándards para los hiperparámetros.\n",
    "\n",
    "Para este algoritmo se utilizó el simulador de pacman anterior, *portado* al entorno de OpenAI Gym, que facilita en gran medida el manejo de transiciones y pruebas con el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![input-to-nn-channel-combined](./img/snaps-0-1-2-3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando las siguientes celdas puede entrenarse o testear un agente A3C con varios procesos asincrónicos simultáneos.\n",
    "\n",
    "Es importante notar que la cantidad de procesos es **una base clave** en este algoritmo, que **utiliza procesos de bajos recursos** que no precisan una GPU, y pueden ejecutarse en paralelo **en un simple CPU**.\n",
    "\n",
    "De esta manera se tienen updates de data iid casi al mismo tiempo, convirtiendo el proceso on-line de actualización de parámetros, en algo más cercano a **aprendizaje supervisado**.\n",
    "\n",
    "**Nota al márgen:** Este algoritmo (como DQN) fue diseñado tomando como referencia la **gran eficiencia con los datos de algoritmos de aprendizaje supervisado** y sus excelentes capacidades de convergencia (a partir de buenos datos), por lo que es correcto relacionarlos de esta forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T23:39:47.847030Z",
     "start_time": "2019-07-15T23:39:47.839611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original directory:\n",
      " /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/\n",
      "Moved to:\n",
      " /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman\n"
     ]
    }
   ],
   "source": [
    "# Change dir to run python files in subdirectory\n",
    "import os\n",
    "os.chdir('../')\n",
    "original_dir=os.getcwd()\n",
    "try:\n",
    "    if original_dir:\n",
    "        os.chdir(original_dir)\n",
    "    gym_pacman_dir = './gym_pacman'\n",
    "    os.chdir(gym_pacman_dir)\n",
    "    original_dir = os.getcwd()[:2-len(gym_pacman_dir)]\n",
    "    print(\"Original directory:\\n\", original_dir)\n",
    "    print(\"Moved to:\\n\", os.getcwd())\n",
    "except:\n",
    "    print(\"Couldn't change to ./gym_pacman folder. Are you already there?\")\n",
    "    print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:40:56.500780Z",
     "start_time": "2019-07-16T21:40:56.496932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman/')\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-16T21:40:55.553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent progress details are displayed on this jupyter notebook terminal (not here)\n",
      "\n",
      "Creating training environment for layout: random_mnih2016\n",
      "Loading previous weights for random_mnih2016... Done.\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(beta =0.01,\n",
    "                 gamma=0.99,\n",
    "                 tau=1.0,\n",
    "                 lr=1e-4,\n",
    "                 layout='random_mnih2016',\n",
    "                 load_previous_weights=True,\n",
    "                 num_processes=16,\n",
    "                 num_processes_to_render=1,\n",
    "                 use_gpu=False,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e6,\n",
    "                 num_local_steps=50,\n",
    "                 save_interval=50,\n",
    "                 saved_path='trained_models',\n",
    "                 log_path='tensorboard')\n",
    "print(\"Agent progress details are displayed on this jupyter notebook terminal (not here)\\n\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acerca de la implementación\n",
    "\n",
    "Se siguió el modelo de [Mnih et al.](https://arxiv.org/abs/1602.01783 \"'Asynchronous Methods for Deep Reinforcement Learning' paper\") para la construcción del algoritmo, partiendo de una una [implementación similar ya programada](https://github.com/vietnguyen91/Super-mario-bros-A3C-pytorch \"vietnguyen91 repository\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![actor-critic-model-lstm](./img/actor-critic-model-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo consta de:\n",
    "\n",
    "* **2 Convolutional layers**\n",
    "  \n",
    "  La primera convoluciona 16 filtros de 8x8 con *stride* 4\n",
    "  \n",
    "  La segund convoluciona 32 filstros de 4x4 con *stride* 2\n",
    "  \n",
    "  Pueden considerarse como features de *pixel space* a *shape/geometric space* \n",
    "\n",
    "\n",
    "* **1 LSTM layer**\n",
    "\n",
    "  Toma como entrada las *high level features* de las capas convolucionales con 2592 (32x9x9) salidas, y aplica un paso de LSTM devolviendo 256 valores\n",
    "  \n",
    "  Representa un **modelo** (*model*) aproximado del environment, que relaciona temporalmente las *features* convolucionadas.\n",
    "  \n",
    "  \n",
    "* **2 Linear layers**\n",
    "\n",
    "  .\n",
    "  \n",
    "  * 1 para el ***Actor*** con 256 entradas y 5 salidas (*número de acciones*) \n",
    "  \n",
    "    Representan el puntaje para cada acción posible\n",
    "  \n",
    "    Los 5 valores **no están acotados**.\n",
    "    \n",
    "    * Para obtener la política se aplica ***Softmax*** sobre ellos.\n",
    "    \n",
    "    * Para calcular la ***entropía*** se utiliza también ***Log-Softmax*** (pues es necesario log_policy)\n",
    "    \n",
    "  .\n",
    "  \n",
    "  * 1 para el ***Critic*** con 256 entradas y 1 sola salida \n",
    "  \n",
    "    Representa ***el valor*** del estado dado (siguiendo la política actual)\n",
    "    \n",
    "    Se utiliza tanto para estimar el valor del **estado actual**, como hacer *bootstraping* y estimar también el valor del **siguiente estado**.\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificaciones particulares:\n",
    "\n",
    "#### Input\n",
    "\n",
    "En el paper original se utiliza como input una secuencia de **4 frames *ordenados*** del simulador de Atari (con cierto skipping por su alto frame rate).\n",
    "\n",
    "Cada frame es reducido a escala de grises, y a un tamaño de 84x84 px.\n",
    "\n",
    "![input-to-nn-channel-0](./img/snaps-0.gif)\n",
    "\n",
    "Para este TP se implementó el mismo preprocesamiento de color (*channels*) y *tamaño* (*shape*) de entrada, pero los 4 frames **son el mismo**, rotado 90° 4 veces (sin skipping, pues el frame rate es mucho menor)\n",
    "\n",
    "![input-to-nn-channel-vertical-stack](./img/snaps-vertical.gif)\n",
    "\n",
    "El objetivo fue brindar más información en este aspecto al algoritmo, que en cada paso deberá usar **los mismos filtros** convolucionales para procesar **las 4 rotaciones** del frame.\n",
    "\n",
    "De esta forma se busca generalizar mejor al espacio del juego, y compensar posibles deficiencias del algoritmo en cuanto a **rotaciones**, que suelen ser un punto débil en modelos de *computer vision*.\n",
    "\n",
    "Detalles del código en: [./gym_pacman/src/env.py](./gym_pacman/src/env.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout:\n",
    "\n",
    "Se agregaron capas de dropout entre  capas convolucionales y lineales originales para (de una forma muy general) **evitar *overfitting*** y no depender del tamaño del modelo.\n",
    "\n",
    "Ya que la data obtenida será en función de la política, que depende directamente del modelo, se decidió por usar un *rate* de **dropout aleatorio**, con valores **entre 0 y 0.2** para las **capas convolucionales** y **entre 0 y 0.6** para las **capas lineales**, a partir de una distribución uniforme.\n",
    "\n",
    "Se espera que dropout ayude a **estabilizar la *importancia*** de los pesos, pero que siga siendo posible tener un modelo mayor (dropout reducido) que genere un **comportamiento más apto**, y así, **mejor data** de la cual aprender.\n",
    "\n",
    "Detalles del código en: [./gym_pacman/src/model.py](./gym_pacman/src/model.py)\n",
    "\n",
    "Más sobre dropout en convolutional layers:\n",
    "[./resources/Dropout_ACCV2016.pdf](./resources/Dropout_ACCV2016.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dive into code\n",
    "\n",
    "Los archivos *train.py* y *process.py* contienen la mayor parte del código correspondiente al algoritmo A3C.\n",
    "\n",
    "En los mismos **fue comentada cada linea de código** a modo puente entre la teoría mencionada anteriormente, y la manera práctica de implementarla.\n",
    "\n",
    "*train.py*: [./gym_pacman/train.py](./gym_pacman/train.py)\n",
    "\n",
    "*/src/process.py*: [./gym_pacman/src/process.py](./gym_pacman/src/process.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem to solve\n",
    "\n",
    "Se experimentó con variantes de distintos mapas de pacman, con distintas características:\n",
    "\n",
    "* Cantidad de pellets\n",
    "* Cantidad de fantasmas\n",
    "* Agresividad de fantasmas\n",
    "* Tamaño del mapa\n",
    "\n",
    "Se decidió por usar un generador de **mapas aleatorios cuadrados**, que modifica el órden del contenido en cada llamada.\n",
    "\n",
    "La **cantidad de fantasmas** es un valor **aleatorio** entre 1 y 3.\n",
    "\n",
    "Similarmente la **agresividad** de cada uno es **aleatoria**.\n",
    "\n",
    "Se probó con tamaños de mapa aleatorios, pero **no se obtuvieron buenos resultados**, muy distinto a tamaños fijos donde pacman *aprendió* buenas dinámicas de juego rápidamente evitando los fantasmas y comiendo cada pellet (con 1 o más).\n",
    "\n",
    "La idea de agregar aleatorieidad a las variables fue para tener un **amplio rango de dificultades**, de modo que en un principio sea posible hayar soluciones a \"niveles\" con dificultades bajas, pero **manteniendo el *stream* data de observaciones de dificultades superiores** a modo de evitar overfitting o máximos locales de los cuales sea muy costoso (o imposible) salir.\n",
    "\n",
    "Se puede pensar como ***curriculum***, donde eventualmente el agente encuentre el orden de resolución más adecuado, haciendo uso de lo ya aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias de modelado\n",
    "\n",
    "Una de las mayores dificultades en el diseño del algoritmo fue determinar cuándo estaba funcionando, y cuándo no.\n",
    "\n",
    "Los resultados suelen tardar horas en aparecer, y los mismos **no son claros** indicativos de si estamos avanzando, estancados en un máximo local, o simplemente en una pésima política.\n",
    "\n",
    "Para ésto se verificó que:\n",
    "\n",
    "* las **entradas** a la red neuronal sean las correctas observando directamente cada matriz y sus transformaciones\n",
    "\n",
    "  Cada frame y rotación fue directamente exportado y guardado como imagen, que son las que se ven a continuación:\n",
    "  \n",
    "![input-to-nn-channel-combined](./img/snaps-0-1-2-3.gif)\n",
    "\n",
    "\n",
    "* en cada paso los valores estén en rangos *aceptables* a través del tiempo, imprimiendo:\n",
    "  * **Salida** de la red neuronal:\n",
    "    \n",
    "    * Probabilidades del *Actor*\n",
    "    \n",
    "    * Puntaje del estado del *Critic*\n",
    "    \n",
    "  * **Loss** utilizado para actualizar pesos, y sus componentes:\n",
    "  \n",
    "    * Actor loss\n",
    "    \n",
    "    * Critic loss\n",
    "    \n",
    "    * Entropy loss\n",
    "    \n",
    "    Estos últimos con la ayuda de ***tensorboard*** como visualizador en ***plots***.\n",
    "    \n",
    "    Se notó una reducción en los *spikes* a medida que avanzaba la cantidad de episodios.\n",
    "    \n",
    "    Aunque también dió indicios de como estos picos podrían estar contribuyendo a la divergencia del algoritmo, de la misma manera que lo haría un *step size* demasiado grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "![total-loss-plot.png](./img/total-loss-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "![total-loss-plot-zoom.png](./img/total-loss-plot-zoom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mayor contribuyente de estos picos en el *loss* es la Value Function:\n",
    "  \n",
    "  Mientras que el *loss* del *Actor* está en el rango de los 200-300 máximo, el *loss* del *Critic* está un órden de magnitud por encima con ~5-10k\n",
    "    \n",
    "  Por eso se decidió poner un *tope* en el máximo valor de *loss* para el *Critic*, determinado como $\\frac{1}{step-size}$\n",
    "    \n",
    "  No queda claro si el comportamiento es parte del algoritmo, o es una falla en la implementación que debería ser controlada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVISIÓN: iDEAS\n",
    "\n",
    "1. Entrenar una red convolucional (feature map) a medida que realizamos updates **luego de cada episodio** , desde *ruido* , y utilizando el **reward** ($G_t^{(n)}$) como objetivo parece ser un **enorme desperdicio de recursos**.\n",
    "  \n",
    "  Si el motivo de la red convolucional es generar una representación geométrica del estado observado, a modo de *facilitar* la estructura de los píxeles (matríces de números) enviados como entrada, por qué se actualizan estos pesos una sola vez por pasada (*step*)?\n",
    "  \n",
    "  Podrían tomarse los primeros 10-100 frames y entrenar las capas convolucionales con **aprendizaje no-supervisado** para generar filtros que generalicen a *visualizaciones* (representación en imágen del estado) similares. \n",
    "  \n",
    "  O si se prioriza mantener el diseño, entrenar las *convolutional layers* con anterioridad de **forma supervisada** con algún dataset con labels, o directamente, usar solo las primeras capas de un **modelo pre-entrenado**.\n",
    "  \n",
    "2. Replay buffer / Memoria. Similarmente a 1., hay poca eficiencia con el uso de datos.\n",
    "  \n",
    "  La capa LSTM da una noción de **estructura de tiempo** al modelo con respecto a los filtros de las convoluciones, pero **no es** el tipo de memoria **a partir de muestras reales**, sino una **estimación aproximada**.\n",
    "  \n",
    "  Usar replay buffer compensaría la falta de información al comienzo del aprendizaje, aumentando la velocidad de convergencia.\n",
    "  \n",
    "  No queda claro si en instancias más avanzadas del aprendizaje, sea relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El objetivo de este trabajo, como el título da un indicio, fue mostrar una secuencia de algoritmos en el que cada uno solucione ciertas falencias del anterior (para problemas grandes) hasta concluir con algo robusto como A3C, que hace uso tanto de una ***Value Function*** como de una ***Policy Function***.\n",
    "\n",
    "Lejos está de ser una lista extensiva, pero da una idea qué tanto ha avanzado el área, al mismo tiempo que cuántos interrogantes faltan por resolver.\n",
    "\n",
    "He notado en el proceso de investigación, implementación y pruebas, que varios conceptos ganaron un nuevo protagonismo, mientras otros siguen formando parte como piezas de algo aún mayor.\n",
    "\n",
    "Todavía existe una gran dependencia en cuanto a *calibrar* los componentes iniciales.\n",
    "\n",
    "Por un lado, ésto se da por la **inmensa complejidad** de los problemas a resolver.\n",
    "\n",
    "Pero por otro, queda claro que aún falta **entender** lo que sucede detrás de ellos, y de qué forma pueden ser abordados.\n",
    "\n",
    "La tendencia actual en **Aprendizaje por Refuerzo** busca ser **más general** en cuanto a los problemas a resolver, al mismo tiempo que intenta ser **más eficiente con la data**, haciendo uso de los avances en hardware para obtener un panorama amplio de posibles candidatos, como también de distintas herramientas teóricas que permitan avanzar en la mejor dirección.\n",
    "\n",
    "Ideas generales sobre A3C siguien vigentes hoy en día, por su orientación hacia hardware solo con CPU (como son común data centers como tiene a su disposición Google), multiprocesos y estabilidad en cuanto a bias inductivo.\n",
    "\n",
    "Sistemas más actuales como [IMPALA con V-Trace](https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/) (off-policy) buscan ir aún más lejos aumentando y dividiendo los distintos procesos **en distintos CPUs**, y **optimizar de manera central con un GPU**, diminuyendo el tiempo de aprendizaje dos o más ordenes de magnitud, generalizando a problemás más complicados (como laberintos en 3 dimensiones con toma de decisiones en desafíos a completar).\n",
    "\n",
    "Es un buen momento para empaparse en **Reinforcement Learning**, aprovechando la posibilidad de poder **experimentar en hardware corriente**, conociendo la **dirección actual del research** en el área por los grandes actores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T00:33:21.946092Z",
     "start_time": "2019-07-16T00:33:15.869209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't change to ./cs188x_pacman folder. Are you already there?\n",
      "Current directory: /home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/cs188x_pacman\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Change dir to run python files in subdirectory\n",
    "import os\n",
    "#os.chdir('../')\n",
    "try:\n",
    "    cs188x_pacman_dir = './cs188x_pacman'\n",
    "    os.chdir(cs188x_pacman_dir)\n",
    "    original_dir = os.getcwd()[:2-len(cs188x_pacman_dir)]\n",
    "    print(\"Original directory:\\n\",original_dir)\n",
    "    print(\"Moved to:\\n\", os.getcwd())\n",
    "except:\n",
    "    print(\"Couldn't change to ./cs188x_pacman folder. Are you already there?\")\n",
    "    print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "#import util\n",
    "#import game\n",
    "#import pacman\n",
    "\n",
    "from pacman import  readCommand, runGames\n",
    "\n",
    "prefs = ['--pacman',     'KeyboardAgent',\n",
    "         '--layout',     'originalClassic']\n",
    "\n",
    "args = readCommand(prefs)\n",
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fourth and last week working on this project with only 2 slow and divergent A3C processes because GPU limitations, to realize that if turning of GPU and only use CPU, I can run more than 16 processes at once\n",
    "\n",
    "![monkey](https://media.giphy.com/media/l4FGFT5D4NKA9rGxy/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.967px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
