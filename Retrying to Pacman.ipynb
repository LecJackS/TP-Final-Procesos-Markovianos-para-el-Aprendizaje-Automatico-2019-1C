{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrying to Pac-man\n",
    "\n",
    "Esta notebook es una continuación del trabajo final de *Procesos Markovianos para el Aprendizaje Automatico (2019)*.\n",
    "\n",
    "> * TP Final - Procesos Markovianos para el Aprendizaje Automatico - 2019 1C\n",
    ">\n",
    "> https://github.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Retrying-to-Pac-man\" data-toc-modified-id=\"Retrying-to-Pac-man-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Retrying to Pac-man</a></span></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Statistics</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Experimento-&quot;learning-rate&quot;\" data-toc-modified-id=\"Experimento-&quot;learning-rate&quot;-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Experimento \"<em>learning-rate</em>\"</a></span></li><li><span><a href=\"#Experimento-&quot;número-de-procesos&quot;\" data-toc-modified-id=\"Experimento-&quot;número-de-procesos&quot;-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Experimento \"<em>número de procesos</em>\"</a></span></li></ul></li></ul></li><li><span><a href=\"#Rewards-and-Expected-Reward-Count\" data-toc-modified-id=\"Rewards-and-Expected-Reward-Count-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Rewards and <em>Expected Reward Count</em></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Sobre-el-código\" data-toc-modified-id=\"Sobre-el-código-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Sobre el código</a></span></li></ul></li></ul></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Optimizer</a></span></li><li><span><a href=\"#Entrada\" data-toc-modified-id=\"Entrada-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Entrada</a></span></li><li><span><a href=\"#Skip-Frame\" data-toc-modified-id=\"Skip-Frame-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Skip Frame</a></span></li><li><span><a href=\"#LSTM-+-Reciclaje-+-Finetunning\" data-toc-modified-id=\"LSTM-+-Reciclaje-+-Finetunning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>LSTM + Reciclaje + <em>Finetunning</em></a></span></li><li><span><a href=\"#Clamp-Critic-(Value-Function)\" data-toc-modified-id=\"Clamp-Critic-(Value-Function)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Clamp Critic (Value Function)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cotas-dinámicas\" data-toc-modified-id=\"Cotas-dinámicas-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Cotas dinámicas</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Problema:\" data-toc-modified-id=\"Problema:-8.1.0.1\"><span class=\"toc-item-num\">8.1.0.1&nbsp;&nbsp;</span>Problema:</a></span></li><li><span><a href=\"#Solución:\" data-toc-modified-id=\"Solución:-8.1.0.2\"><span class=\"toc-item-num\">8.1.0.2&nbsp;&nbsp;</span>Solución:</a></span></li></ul></li><li><span><a href=\"#Normalizando-recompensas\" data-toc-modified-id=\"Normalizando-recompensas-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Normalizando recompensas</a></span></li></ul></li></ul></li><li><span><a href=\"#Plots\" data-toc-modified-id=\"Plots-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clamp-on-loss\" data-toc-modified-id=\"Clamp-on-loss-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Clamp on loss</a></span></li><li><span><a href=\"#Clamp-on-Critic-value\" data-toc-modified-id=\"Clamp-on-Critic-value-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Clamp on Critic value</a></span></li><li><span><a href=\"#Tanh-+-Normalization\" data-toc-modified-id=\"Tanh-+-Normalization-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Tanh + Normalization</a></span></li><li><span><a href=\"#Tanh-+-Normalization-w/-maxGt-decay\" data-toc-modified-id=\"Tanh-+-Normalization-w/-maxGt-decay-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Tanh + Normalization w/ maxGt decay</a></span></li></ul></li><li><span><a href=\"#Resultados\" data-toc-modified-id=\"Resultados-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Resultados</a></span></li><li><span><a href=\"#Código\" data-toc-modified-id=\"Código-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Código</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Test</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Video:\" data-toc-modified-id=\"Video:-11.1.0.1\"><span class=\"toc-item-num\">11.1.0.1&nbsp;&nbsp;</span>Video:</a></span></li></ul></li></ul></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Train</a></span></li></ul></li><li><span><a href=\"#Berkeley-Pac-man\" data-toc-modified-id=\"Berkeley-Pac-man-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Berkeley Pac-man</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Entrada-a-la-red-neuronal\" data-toc-modified-id=\"Entrada-a-la-red-neuronal-12.0.1\"><span class=\"toc-item-num\">12.0.1&nbsp;&nbsp;</span>Entrada a la red neuronal</a></span></li></ul></li></ul></li><li><span><a href=\"#Código\" data-toc-modified-id=\"Código-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Código</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Conclusión\" data-toc-modified-id=\"Conclusión-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Conclusión</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los meses pasados, luego de la entrega, seguí realizando pruebas para asegurarme que el algoritmo funcione (algo que **no** fue claramente demostrado en la entrega anterior).\n",
    "\n",
    "Para eso se utilizó  [un nuevo environment de pacman (Ms Pac-man)](https://gym.openai.com/envs/MsPacman-v0/) que permitiera realizar experimentos a mayor velocidad.\n",
    "\n",
    "![mspacman.png](./img/mspacman.png)\n",
    "\n",
    "De esta forma se pudieron realizar ajustes que **demostraron buenos resultados** en el juego, **generalizando también** al lento pacman de Berkeley.\n",
    "\n",
    "A continuación, una lista de correcciones y resultados obtenidos a partir de este nuevo entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "Una **prioridad** para esta serie de experimentos fue tener un **mayor grado de información** en cuanto a los resultados de distintos episodios en el tiempo.\n",
    "\n",
    "Para eso se agregaron las siguientes estadísticas:\n",
    "\n",
    "* **Promedio (mean)** de rewards acumulados en **últimos 100 episodios**\n",
    "* **Mediana** del reward acumulado en los **últimos 100 episodios**\n",
    "* **Desviación estándar** de rewards acumulados en últimos **100 episodios**\n",
    "* ***\"Mediana-estándar\"***, como la mediana menos la media sobre desviación estándar $\\frac{\\text{median}-\\text{mean}}{\\sigma}$.\n",
    "  \n",
    "  A modo de combinar en un mismo plot los rewards promedios por cantidad (mediana) y por valor (mean).\n",
    "  \n",
    "\n",
    "### Experimento \"*learning-rate*\"\n",
    "\n",
    "Se compararon 3 valores de step size diferentes:\n",
    "\n",
    "* lr: $10^{-3}$ - ***Rojo***\n",
    "* lr: $10^{-4}$ - ***Naranja***\n",
    "* lr: $10^{-5}$ - ***Azul***\n",
    "\n",
    "![comp-lr.png](./img/comp-lr.png)\n",
    "\n",
    "![comp-lr-smooth.png](./img/comp-lr-smooth.png)\n",
    "\n",
    "Ésto dió bastante certeza en que usar un lr de $10^{-4}$ como se venía haciendo, era lo indicado.\n",
    "\n",
    "Búsquedas más finas no mostraron grandes diferencias, aunque fueron pruebas no tan extensivas en el tiempo.\n",
    "\n",
    "### Experimento \"*número de procesos*\"\n",
    "\n",
    "Un hyperparámetro que agrega A3C es la **cantidad de procesos asincrónicos** a ejecutar.\n",
    "\n",
    "Se comparó **1 vs 6 procesos durante varios días**.\n",
    "\n",
    "Los resultados fueron en favor de una mayor cantidad de procesos.\n",
    "\n",
    "![comp-num-procesos.png](./img/comp-num-procesos.png)\n",
    "(azul: 6; gris: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards and *Expected Reward Count*\n",
    "\n",
    "Como Vlad Mnih menciona en el siguiente video, simplificar los rewards *recortándolos* en valores estándar como -1,0,1 **facilita** la solución del problema para el agente en muchos casos, ya que **convierte el objetivo en obtener más cantidad de veces** rewards, distinto a *mayor reward*, donde existen distintos valores posible para el reward (ej. pacman: +500, +10, +1, 0, -1, -500), y sus combinaciones al calcular $G_t$ puede dar valores parecidos, a partir de combinaciones muy diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T19:44:42.540462Z",
     "start_time": "2019-08-12T19:44:42.532537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-mhBD8Frkc4?start=5520\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-mhBD8Frkc4?start=5520\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
    "#@1:32:00 https://youtu.be/-mhBD8Frkc4?t=5520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otras palabras, se **maximiza la esperanza de la *cantidad* de rewards recibidos** (*expected reward count*).\n",
    "\n",
    "|   +500  |  +10   |  0 |  -1 |    -500|\n",
    "|------|------|------|------|------|\n",
    "|  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |\n",
    "|   +1  | +1 |   0  |0|   0  | 0 | \n",
    "\n",
    "Solo esta modificación en el código produjo rápidos resultados, lo cuál tiene sentido para pacman, donde cada pellet y ganar el juego es ahora un +1, y el maximo puntaje será comer todas las pellets (y los fantasmas si *scared*).\n",
    "\n",
    "![pacman-episode-1.gif](./img/pacman-episode-1.gif)\n",
    "\n",
    "A pesar de haber aprendido bastante bien a recoger las pellets de un mapa de 14x14, demuestra poca cautela con los fantasmas, lo que indicó que podría ser conveniente agregar un valor negativo para el evento en el que hace contacto, y pierde.\n",
    "\n",
    "|   +500  |  +10   |  0 |  -1 |    -500|\n",
    "|------|------|------|------|------|\n",
    "|  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |\n",
    "|   +1  | +1 |   0  |  0  | -1 | \n",
    "\n",
    "Esta modificación no produjo ningún cambio en el comportamiento de Pac-man, lo que indica la necesidad de una mayor exploración en los valores de los hiperparámetros, o solo más tiempo de entrenamiento.\n",
    "\n",
    "### Sobre el código\n",
    "\n",
    "Gym contiene un conveniente paquete de *Wrappers* que permiten envolver al *environment* luego de creado con *gym.make('MsPacman-v0')* modificando funciones como *environment.step(action)* para que devuelva un reward modificado desde cualquier otro lado que sea llamada: \n",
    "\n",
    ">    class UnitReward_cs188x(Wrapper):\n",
    ">        def __init__(self, env=None, monitor=None):\n",
    ">            super(UnitReward_cs188x, self).__init__(env)\n",
    ">            \n",
    ">        def step(self, action):\n",
    ">            state, reward, done, info = self.env.step(action)\n",
    ">            if reward > 0:\n",
    ">                reward = 1.\n",
    ">            else:\n",
    ">                reward = 0.\n",
    ">\n",
    ">            return state, reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "Se implementó ***RMSProp con parámetros compartidos*** para contrastar con los de ***Adam***, previamente utilizado.\n",
    "\n",
    "No se observó ninguna diferencia en largos experimentos con los mismos hyperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrada\n",
    "\n",
    "Se modificó el tipo de ***entrada a la red neuronal***.\n",
    "\n",
    "* Manteniendo las **4 rotaciones** de 90°\n",
    "* Asignando a cada rotación, los valores (intensidades) de su **canal de color correspondiente (RGB)**, y *grayscale* para la última imagen.\n",
    "\n",
    "![pacman-episode-4.gif](./img/pacman-episode-4.gif)\n",
    "\n",
    "Originalmente se pensó como alternativa a buscar manualmenmte el mejor equilibrio de grises para la red (variando mean, varianza, etc).\n",
    "\n",
    "Las primeras pruebas fueron usando ***ms-pacman de Atari***, sin las rotaciones (ya que tiene información extra en la parte inferior de la pantalla) pero usando el mismo tipo de división de colores:\n",
    "\n",
    "![mspacman-episode-4.gif](./img/mspacman-episode-4.gif)\n",
    "\n",
    "Partiendo de **pesos ya entrenados** solo con escala de grises, el agente mostró una rápida adaptación al nuevo tipo de entrada:\n",
    "\n",
    "![learning-after-colours.png](./img/learning-after-colours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Frame\n",
    "\n",
    "Otra característica mencionada pero poco explorada en el anterior proyecto, fue el uso de ***skip frame***.\n",
    "\n",
    "La primera impresión al ejecutar ms-pacman con el mismo modelo anterior, fue de que **la simulación también era lenta**.\n",
    "\n",
    "El motivo fue el **alto frame rate**, con el cual cada frame era un estado que debía procesarse por la NN.\n",
    "\n",
    "Para eso se utiliza ***skip frame*** que elije, combina y/o saltea frames consecutivos.\n",
    "\n",
    "Para DQN, la entrada a la red neuronal fueron 4 frames apilados:\n",
    "\n",
    "    [max(T-1,  T),\n",
    "     max(T+3,  T+4),\n",
    "     max(T+7,  T+8),\n",
    "     max(T+11, T+12)]\n",
    "     \n",
    "Donde `max(T-1, T)` es el máximo *elementwise* entre las matrices de píxeles de **dos frames consecutivos**.\n",
    "\n",
    "**Descartando otros 2** frames luego de cada operación max().\n",
    "\n",
    "\n",
    "Para pacman de Atari, se utilizó un filtro similar al de DQN, con una diferencia:\n",
    "\n",
    "* con 1 solo *skip frame*\n",
    "\n",
    "        [max(T,    T+1),\n",
    "         max(T+3,  T+4),\n",
    "         max(T+6,  T+7),\n",
    "         max(T+9,  T+10)]\n",
    "\n",
    "* Sin *skip frames*, solo máximo entre cada dos\n",
    "\n",
    "        [max(T,    T+1),\n",
    "         max(T+2,  T+3),\n",
    "         max(T+4,  T+5),\n",
    "         max(T+6,  T+7)]\n",
    "\n",
    "La siguiente imagen muestra la entrada de píxeles de los 4 estados a la red neuronal, calculando máximo entre cada dos, y salteando 1 frame.\n",
    "\n",
    "![dqn-skip-frame.png](./img/dqn-skip-frame.png)\n",
    "\n",
    "Se determinó que **el frame descartado limitaba la capacidad de reacción del agente**, por lo que se decidió por no usar skip frames.\n",
    "\n",
    "El cambio de entrada produjo resultados negativos (esperados) al agente **ya entrenado** con la entrada anterior.\n",
    "\n",
    "En unas horas de entrenamiento volvió a los valores anteriores y los siguió superando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Reciclaje + *Finetunning*\n",
    "\n",
    "Una vez entrenado el agente de modelo neuronal **lineal**, se decidió probar nuevamente una capa LSTM en su lugar.\n",
    "\n",
    "Para ello:\n",
    "\n",
    "1. Se **cargaron los pesos ya entrenados** en el modelo **lineal**\n",
    "2. **Se *\"congelaron\"* las capas convolucionales** para mantener los filtros/features aprendidos\n",
    "3. **Se entrenó el resto** del modelo neuronal **hasta ver mejoras** en el resultado\n",
    "4. Se descongelaron todas las capas y siguió normalmente con el entrenamiento en conjunto\n",
    "\n",
    "A continuación, el nuevo modelo neuronal con LSTM y las capas convolucionales congeladas siendo entrenado, partiendo de los pesos del modelo lineal:\n",
    "\n",
    "![lstm-learn.png](./img/lstm-learn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least\n",
    "# Clamp Critic (Value Function)\n",
    "\n",
    "En la sección [\"Estrategias de modelado\"](https://github.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-/blob/master/Very%20quick%20roadmap%20to%20Asynchronous%20Advantage%20Actor%20Critic.ipynb#Estrategias-de-modelado) del trabajo anterior, se notaron **picos muy grandes** en el error (*loss*) calculado, por lo que se optó por poner topes en el mismo de $\\frac{1}{\\text{step-size}}$, sin conocer el motivo de ellos.\n",
    "\n",
    "El problema real fue que la **Value Function (*Critic*)** era una salida en la red neuronal que entregaba valores reales, sin cotas de ningún tipo.\n",
    "\n",
    "Recordando el modelo:\n",
    "\n",
    "![actor-critic-model.png](./img/actor-critic-model.png)\n",
    "\n",
    "Dado que el error total se calcula como:\n",
    "\n",
    ">    total_loss = -actor_loss + ***critic_loss*** - opt.beta * entropy_loss\n",
    "\n",
    "Donde *critic_loss* es:\n",
    "\n",
    ">    critic_loss = critic_loss + (***(Gt - value)^2***) / 2\n",
    "\n",
    "Esa **diferencia al cuadrado** ($(G_t - V_t)^2$)  era la causante de tales magnitudes.\n",
    "\n",
    "Una solución encontrada, fue **acotar los valores de la value function** $V_t$ entre $\\left[0,\\, 20\\right]$.\n",
    "\n",
    "La magnitud $20$ surge de ver los plots de rewards promedio del juego con valores cercanos a 20, y 30 si casi ganando el episodio.\n",
    "\n",
    "El rango de esa magnitud $G_t$ **será particular de cada juego**, como también **de cada instancia en el tiempo** (distintas para un agente bien entrenado, que para uno random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cotas dinámicas\n",
    "\n",
    "Una mejor solución fue determinar la magnitud de la cota, como el máximo/mínimo Gt obtenido hasta el momento.\n",
    "\n",
    "> **Inicia** con **cotas para el Crítico** de $[-1, \\,1]$.\n",
    ">\n",
    "> Al finalizar cada episodio ($n$ steps), **se calcula Gt** de los datos guardados\n",
    ">\n",
    "> $G_t^{(n)} = R_{t+1} + \\gamma\\, R_{t+2}\\, + \\gamma^2\\, R_{t+3} + \\, ... \\, + \\, \\gamma^{n-1} R_{t+n} + \\gamma^{n}\\, \\tilde{V}(S_{t+n})$\n",
    ">\n",
    ">     1. Si (Gt > cotaMax): \n",
    ">\n",
    ">        cotaMax = Gt\n",
    ">\n",
    ">     2. Si (Gt < cotaMin): \n",
    ">\n",
    ">        cotaMin = Gt\n",
    "\n",
    "Luego, estimo $V_t$ usando red neuronal, y recorto con cotas:\n",
    "\n",
    ">      logits, value = local_model(state)\n",
    ">\n",
    ">      value.clamp(cotaMin, cotaMax)\n",
    "\n",
    "De esta forma, los valores máximo/mínimo del Crítico aumentarán **a medida que se vean puntajes más altos**.\n",
    "\n",
    "#### Problema:\n",
    "\n",
    "* Las **cotas no forman parte del modelo** neuronal, el cual tendrá **como salida valores no-acotados**, que se recortarán, y **a partir de estos valores modificados se calculará el error** (***loss***) con el cual se actualizarán los pesos.\n",
    "\n",
    "#### Solución:\n",
    "\n",
    "* Agregar una ***función tangente hiperbólica (tanh)*** al modelo neuronal, de forma que a la salida sean valores entre -1 y 1, con un mejor gradiente con el cual actualizar los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando recompensas\n",
    "\n",
    "Tener una salida entre $-1$ y $+1$ permite simplificar el problema devolviendo estimaciones cercanas a $+1$ para estados favorables, y a $-1$ para los que no.\n",
    "\n",
    "Es necesario ahora normalizar acorde el resto de las recompensas, que siguen siendo valores entre $0$ y $\\approx30$.\n",
    "\n",
    "Para ello, cada reward $R_{t+1}$ obtenido luego de cada acción $A_t$ es dividido por el máximo $G_t$ visto hasta el momento\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    reward = reward / max_Gt\n",
    "    \n",
    "Siendo:\n",
    "    \n",
    "    R = reward + gamma * R\n",
    "    max_Gt = max(max_Gt, abs(R))\n",
    "    \n",
    "Con esto se soluciona tanto el problema de los **valores extremos del Crítico**, como también la **diferencia en órden de magnitud del error del Actor**:\n",
    "\n",
    "$$loss_{critic} = loss_{critic} + \\frac{1}{2}((G_t - V_t)^ 2)$$\n",
    "\n",
    "Ya que:\n",
    "\n",
    "$V_t \\in [-1, 1]$ , pues es la imagen de una Tanh\n",
    "\n",
    "y \n",
    "\n",
    "$G_t \\in [-1, 1]$ , pues es siempre dividido por el ***máximo*** $G_t$ observado hasta el momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "de los errores calculados por los 3 métodos mencionados.\n",
    "\n",
    "**Observar:** Escalas en el eje Y (error)\n",
    "\n",
    "## Clamp on loss \n",
    "![loss-before-clamp.png](./img/loss-before-clamp.png)\n",
    "\n",
    "## Clamp on Critic value\n",
    "![loss-after-clamp.png](./img/loss-after-clamp.png)\n",
    "\n",
    "## Tanh + Normalization\n",
    "![total-loss-normalized.png](./img/total-loss-normalized.png)\n",
    "\n",
    "## Tanh + Normalization w/ maxGt decay\n",
    "\n",
    "    # on each time step, decay max_Gt over time to adjust to actual Gt scale\n",
    "    max_Gt = max_Gt * 0.9999\n",
    "    \n",
    "\n",
    "          \n",
    "![total-loss-decay.png](./img/total-loss-decay.png)\n",
    "\n",
    "Formado por la suma de:\n",
    "\n",
    "1. **+ Critic** loss\n",
    "2. **- Actor** loss\n",
    "3. **- Entropy** loss * 0.01\n",
    "\n",
    "![critic-loss-decay.png](./img/critic-loss-decay.png)\n",
    "![actor-loss-decay.png](./img/actor-loss-decay.png)\n",
    "![entropy-loss-decay.png](./img/entropy-loss-decay.png)\n",
    "\n",
    "  - `max_Gt` con valor inicial `max_Gt=10`, con `decay=0.9999`:\n",
    "![maxGt-decay.png](./img/maxGt-decay.png)\n",
    "\n",
    "Plots en orden de aparición:\n",
    "\n",
    "  1. `Acum_Reward`: **Recompensa** por cada episodio **sin normalizar**\n",
    "  2. `Gt`: **Recompensa** por cada episodio **normalizada** con `max_Gt`\n",
    "  3. `Last100_mean`: **Recompensa promedio** de los últimos 100 ***episodios***\n",
    "  4. `Last100_median`: **Mediana** de las últimas 100 ***recompensas***\n",
    "\n",
    "![returns-decay.png](./img/returns-decay.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "Este último cambio volvió al algoritmo más **robusto** y **predecible**, pudiendo ajustar mejor los hiperparámetros al ritmo de avances en el aprendizaje (mayores puntajes promedio y mediana)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código\n",
    "\n",
    "## Test\n",
    "\n",
    "El siguiente código muestra un agente **entrenado durante varios días** en *Ms Pac-man de Atari*.\n",
    "\n",
    "Si es la primera vez que se ejecuta el código, descargará un archivo el archivo de 2.7MB con los pesos guardados (dos celdas más abajo se puede en video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T17:53:57.502962Z",
     "start_time": "2019-09-14T17:53:57.498232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if saved model: ./trained_models/gym-pacman_atari ...\n",
      "Already there.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "saved_path = './trained_models'\n",
    "layout = 'atari'\n",
    "saved_model = \"{}/gym-pacman_{}\".format(saved_path, layout)\n",
    "print(\"Checking if saved model: {} ...\".format(saved_model))\n",
    "if not os.path.isfile(saved_model):\n",
    "    try:\n",
    "        import urllib.request\n",
    "        print('File not found, downloading saved model...')\n",
    "        url = 'https://github.com/LecJackS/trained-models/blob/master/gym-pacman/gym-pacman_atari?raw=true'\n",
    "        file_name = 'gym-pacman_atari'\n",
    "        urllib.request.urlretrieve(url, '{}/{}'.format(saved_path, file_name))\n",
    "        print('Download done.')\n",
    "    except:\n",
    "        print(\"Something wrong happened, couldn't download model\")\n",
    "else:\n",
    "    print('Already there.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El juego consiste en devorar todas las pellets teniendo 3 vidas disponibles, siguiendo las [reglas usuales de Pac-man](https://en.wikipedia.org/wiki/Pac-Man_(Atari_2600)#Gameplay).\n",
    "\n",
    "Si el juego parpadea, es porque completó el nivel.\n",
    "\n",
    "$_\\text{(si uno parpadea, se pierde el juego)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T17:54:37.622483Z",
     "start_time": "2019-09-14T17:53:59.063602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting number of NN inputs/outputs for atari\n",
      "Loading saved model: trained_models/gym-pacman_atari ...\n",
      "Done.\n",
      " 1: Done. #returns/#steps: 120 / 191 = 0.62827\n",
      " 2: Done. #returns/#steps: 175 / 244 = 0.71721\n",
      " 3: Done. #returns/#steps: 102 / 153 = 0.66667\n",
      " 4: Done. #returns/#steps: 207 / 304 = 0.68092\n",
      " 5: Done. #returns/#steps: 107 / 186 = 0.57527\n",
      " 6: Done. #returns/#steps: 113 / 186 = 0.60753\n",
      " 7: Done. #returns/#steps: 141 / 217 = 0.64977\n",
      " 8: Done. #returns/#steps: 195 / 300 = 0.65000\n",
      " 9: Done. #returns/#steps: 213 / 330 = 0.64545\n",
      "10: Done. #returns/#steps: 160 / 235 = 0.68085\n",
      "Number of tests completed.\n"
     ]
    }
   ],
   "source": [
    "# Reset kernel to go to default directory\n",
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "#os.chdir('./gym_pacman')\n",
    "\n",
    "from test_agent import test\n",
    "from argparse import Namespace\n",
    "args = Namespace(layout='atari',\n",
    "                 saved_path='trained_models',\n",
    "                 num_games_to_play=10)\n",
    "test(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T21:51:40.361589Z",
     "start_time": "2019-09-13T21:51:40.167671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/OwYbJGBBL64\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fc956219860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('OwYbJGBBL64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "El siguiente código sigue entrenando un agente guardado en la carpeta `trained_models`.\n",
    "\n",
    "Si no existe el archivo con el `layout` solicitado, crea uno nuevo y lo entrena desde cero.\n",
    "\n",
    "Para ver las estadísticas en Tensorboard, correr:\n",
    "\n",
    ">      tensorboard --logdir=alias:./tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T17:56:02.062808Z",
     "start_time": "2019-09-14T17:55:40.663171Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\n",
      "\n",
      "Getting number of NN inputs/outputs for atari\n",
      "Loading previous weights for atari... Done.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-06a8dd58be2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                  record=False)\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "#os.chdir('./gym_pacman')\n",
    "from train import train\n",
    "from argparse import Namespace\n",
    "args = Namespace(beta =0.001,\n",
    "                 gamma=0.9,\n",
    "                 tau=1.0,\n",
    "                 lr=3e-5,\n",
    "                 layout='atari',\n",
    "                 load_previous_weights=True,\n",
    "                 num_processes=8,                # numero de procesos asincronicos\n",
    "                 num_processes_to_render=1,     # numero de procesos a visualizar\n",
    "                 use_gpu=False,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e9,\n",
    "                 num_local_steps=50,     # async updates every\n",
    "                 save_interval=25,\n",
    "                 saved_path='trained_models',\n",
    "                 log_path='tensorboard',\n",
    "                 record=False)\n",
    "print(\"Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\\n\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berkeley Pac-man\n",
    "\n",
    "Usando las mejoras implementadas en Ms Pac-man de Atari, se obtuvieron mejores resultados en el Pac-man de Berkeley.\n",
    "\n",
    "Cada mapa de cada juego (episodio) se genera de forma aleatoria:\n",
    "\n",
    "* Mapa de **14x14** celdas\n",
    "* Cantidad de fantasmas entre **1 y 3**\n",
    "* Cada fantasma $i$ es **creado** con una **agresividad aleatoria** $\\in [0, 0.9]$ (acción aleatoria $A^-_i$ con $P(A^-_i) \\in [1.0, 0.1]$)\n",
    "* ***Pellets (food)*** ocuparán de manera aleatoria una **fracción del espacio** vacío $\\in [0.1, 0.9]$\n",
    "* **Posición de inicio** de todos los agentes es **aleatoria** (con una mínima distancia en favor de Pac-man)\n",
    "\n",
    "### Entrada a la red neuronal \n",
    "\n",
    "* Tensor de dimensión $4 \\times 84 \\times 84$\n",
    "* **Cada imagen** de $84 \\times 84$ **corresponde al mismo frame** (sin *skip-frame*)\n",
    "* Cada uno de los $4$ canales **corresponde a las intensidades** de ***RGB+Grayscale*** de los píxeles del juego en colores\n",
    "\n",
    "![cs188x-pacman-episode.gif](./img/cs188x-pacman-episode.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código\n",
    "\n",
    "## Test\n",
    "\n",
    "El siguiente código muestra un agente **entrenado durante varios días** en el [simulador de Pac-man de Berkeley](http://ai.berkeley.edu/project_overview.html).\n",
    "\n",
    "Si es la primera vez que se ejecuta el código, descargará un archivo el archivo de 2.7MB con los pesos guardados (dos celdas más abajo se puede en video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T18:02:39.181425Z",
     "start_time": "2019-09-14T18:02:39.172083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if saved model: ./trained_models/gym-pacman_atari ...\n",
      "Already there.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "saved_path = './trained_models'\n",
    "layout = 'atari'\n",
    "saved_model = \"{}/gym-pacman_{}\".format(saved_path, layout)\n",
    "print(\"Checking if saved model: {} ...\".format(saved_model))\n",
    "if not os.path.isfile(saved_model):\n",
    "    try:\n",
    "        import urllib.request\n",
    "        print('File not found, downloading saved model...')\n",
    "        url = 'https://github.com/LecJackS/trained-models/blob/master/gym-pacman/gym-pacman_cs188x?raw=true'\n",
    "        file_name = 'gym-pacman_cs188x'\n",
    "        urllib.request.urlretrieve(url, '{}/{}'.format(saved_path, file_name))\n",
    "        print('Download done.')\n",
    "    except:\n",
    "        print(\"Something wrong happened, couldn't download model\")\n",
    "else:\n",
    "    print('Already there.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T18:18:02.502666Z",
     "start_time": "2019-09-14T18:17:48.523464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting number of NN inputs/outputs for layout cs188x\n",
      "Loading saved model: trained_models/gym-pacman_cs188x ...\n",
      "Done.\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "Pacman died! Score: -486\n",
      "PENALTY_:  0\n",
      "real reward: -491.0\n",
      "reward:  -1.0\n",
      " 1: Done. #returns/#steps:   0 /   6 = 0.00000\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 199.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "Pacman died! Score: -141\n",
      "PENALTY_:  0\n",
      "real reward: -491.0\n",
      "reward:  -1.0\n",
      " 2: Done. #returns/#steps:  19 /  41 = 0.46341\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: 9.0\n",
      "reward:  1.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n",
      "PENALTY_:  0\n",
      "real reward: -1.0\n",
      "reward:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-52dd47a71121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0msaved_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'trained_models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                  num_games_to_play=10)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman/test_agent.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hx, cx)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# to flatten all filters: x.view(x.size(0), -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Softmax is applied later, same with log-softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mactor\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset kernel to go to default directory\n",
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "#os.chdir('./gym_pacman')\n",
    "\n",
    "from test_agent import test\n",
    "from argparse import Namespace\n",
    "args = Namespace(layout='cs188x',\n",
    "                 saved_path='trained_models',\n",
    "                 num_games_to_play=10)\n",
    "test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-14T21:44:25.780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\n",
      "\n",
      "Getting number of NN inputs/outputs for layout cs188x\n",
      "Loading previous weights for cs188x... Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "#os.chdir('./gym_pacman')\n",
    "from train import train\n",
    "from argparse import Namespace\n",
    "args = Namespace(beta =0.003,\n",
    "                 gamma=0.9,\n",
    "                 tau=1.0,\n",
    "                 lr=3e-4,\n",
    "                 layout='cs188x',\n",
    "                 load_previous_weights=True,\n",
    "                 num_processes=8,                # numero de procesos asincronicos\n",
    "                 num_processes_to_render=1,     # numero de procesos a visualizar\n",
    "                 use_gpu=False,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e9,\n",
    "                 num_local_steps=100,     # async updates every\n",
    "                 save_interval=25,\n",
    "                 saved_path='trained_models',\n",
    "                 log_path='tensorboard',\n",
    "                 record=False)\n",
    "print(\"Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\\n\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Esta segunda parte del trabajo permitió familiarizarme con varios conceptos que a su vez sirvieron de guía a la hora de corregir y explorar soluciones.\n",
    "\n",
    "* El poder computacional sigue siendo un limitante, sobre todo para los cambios en el algoritmo que requieren modificaciones a los hiperparámetros.\n",
    "\n",
    "\n",
    "* Encontrar soluciones a juegos de Atari es *cool*, pero problemas o juegos más simples formulados con objetivos específicos pueden dar respuestas mucho más velozmente y en la dirección correcta.\n",
    "\n",
    "  Un ejemplo de esto es [*bsuite*](https://github.com/deepmind/bsuite) va en ese camino, como varios otros que aparecen a causa de la demanda actual del área de RL, que buscan estándares sobre los cuales calificar distintos algoritmos.\n",
    "  \n",
    "  \n",
    "* Juegos diferentes pueden ser resueltos con el mismo algoritmo, pero ambos deberán ser entrenados desde cero (no hay transferencia de conocimiento en \"acciones\" generales/básicas).\n",
    "\n",
    "\n",
    "* Implementar estos algoritmos demanda mucho trabajo de calibración para hacer coincidir las partes. \n",
    "\n",
    "  Teniendo formas estándares de evaluar algoritmos, y algoritmos optimizados de cada tipo, sería una buena estrategia partir de éstos últimos para adaptarlos a nuestras necesidades con modicaciones o nuevos algoritmos, siendo eficientes en el manejo del tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final-fantasy.gif](./img/final-fantasy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181.038px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
