{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrying to Pac-man\n",
    "\n",
    "follow up work from:\n",
    "\n",
    "## TP Final - Procesos Markovianos para el Aprendizaje Automatico - 2019 1C\n",
    "\n",
    "https://github.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las pasadas 4 semanas, luego de la entrega, seguí realizando pruebas para asegurarme que el algoritmo funcione (algo que **no** fue claramente demostrado en la entrega anterior).\n",
    "\n",
    "Para eso se utilizó  [un nuevo environment de pacman (mspacman)](https://gym.openai.com/envs/MsPacman-v0/) que permitiera realizar experimentos a mayor velocidad.\n",
    "\n",
    "![mspacman.png](./img/mspacman.png)\n",
    "\n",
    "De esta forma se pudieron realizar ajustes que **demostraron buenos resultados** en el juego, **generalizando también** al lento pacman de Berkeley.\n",
    "\n",
    "A continuación, una lista de correcciones y resultados obtenidos a partir de este nuevo entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "Una **prioridad** para esta serie de experimentos fue tener un **mayor grado de información** en cuanto a los resultados de distintos episodios en el tiempo.\n",
    "\n",
    "Para eso se agregaron las siguientes estadísticas:\n",
    "\n",
    "* **Promedio (mean)** de rewards acumulados en **últimos 100 episodios**\n",
    "* **Mediana** del reward acumulado en los **últimos 100 episodios**\n",
    "* **Desviación estándar** de rewards acumulados en últimos **100 episodios**\n",
    "* ***\"Mediana-estándar\"***, como la mediana menos la media sobre desviación estándar $\\frac{\\text{median}-\\text{mean}}{\\sigma}$.\n",
    "  \n",
    "  A modo de combinar en un mismo plot los rewards promedios por cantidad (mediana) y por valor (mean).\n",
    "  \n",
    "\n",
    "### Experimento \"*learning-rate*\"\n",
    "\n",
    "Se compararon 3 valores de step size diferentes:\n",
    "\n",
    "* lr: $10^{-3}$ - ***Rojo***\n",
    "* lr: $10^{-4}$ - ***Naranja***\n",
    "* lr: $10^{-5}$ - ***Azul***\n",
    "\n",
    "![comp-lr.png](./img/comp-lr.png)\n",
    "\n",
    "![comp-lr-smooth.png](./img/comp-lr-smooth.png)\n",
    "\n",
    "Ésto dió bastante certeza en que usar un lr de $10^{-4}$ como se venía haciendo, era lo indicado.\n",
    "\n",
    "Búsquedas más finas no mostraron grandes diferencias, aunque fueron pruebas no tan extensivas en el tiempo.\n",
    "\n",
    "### Experimento \"*número de procesos*\"\n",
    "\n",
    "Un hyperparámetro que agrega A3C es la **cantidad de procesos asincrónicos** a ejecutar.\n",
    "\n",
    "Se comparó **1 vs 6 procesos durante varios días**.\n",
    "\n",
    "Los resultados fueron en favor de una mayor cantidad de procesos.\n",
    "\n",
    "![comp-num-procesos.png](./img/comp-num-procesos.png)\n",
    "(azul: 6; gris: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards and *Expected Reward Count*\n",
    "\n",
    "Como Vlad Mnih menciona en el siguiente video, simplificar los rewards *recortándolos* en valores estándar como -1,0,1 **facilita** la solución del problema para el agente en muchos casos, ya que **convierte el objetivo en obtener más cantidad de veces** rewards, distinto a *mayor reward*, donde existen distintos valores posible para el reward (ej. pacman: +500, +10, +1, 0, -1, -500), y sus combinaciones al calcular $G_t$ puede dar valores parecidos, a partir de combinaciones muy diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T19:44:42.540462Z",
     "start_time": "2019-08-12T19:44:42.532537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-mhBD8Frkc4?start=5520\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-mhBD8Frkc4?start=5520\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
    "#@1:32:00 https://youtu.be/-mhBD8Frkc4?t=5520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otras palabras, se **maximiza la esperanza de la *cantidad* de rewards recibidos** (*expected reward count*).\n",
    "\n",
    "|   +500  |  +10   |  0 |  -1 |    -500|\n",
    "|------|------|------|------|------|\n",
    "|  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |\n",
    "|   +1  | +1 |   0  |0|   0  | 0 | \n",
    "\n",
    "Solo esta modificación en el código produjo rápidos resultados, lo cuál tiene sentido para pacman, donde cada pellet y ganar el juego es ahora un +1, y el maximo puntaje será comer todas las pellets (y los fantasmas si *scared*).\n",
    "\n",
    "![pacman-episode-1.gif](./img/pacman-episode-1.gif)\n",
    "\n",
    "A pesar de haber aprendido bastante bien a recoger las pellets de un mapa de 14x14, demuestra poca cautela con los fantasmas, lo que indicó que podría ser conveniente agregar un valor negativo para el evento en el que hace contacto, y pierde.\n",
    "\n",
    "|   +500  |  +10   |  0 |  -1 |    -500|\n",
    "|------|------|------|------|------|\n",
    "|  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |  $\\downarrow$  |\n",
    "|   +1  | +1 |   0  |0|   0  | -1 | \n",
    "\n",
    "Con este modelado de rewards se obtuvieron los resultados deseados:\n",
    "\n",
    "* Pac-man comenzó a uir de los fantasmas y a ir más directamente a comer cada pellet (incentivado al terminar el episodio sin completarlo).\n",
    "\n",
    "Los descuentos de $-1$ por time step fueron ignorados, por devolver resultados positivos pero poco interpretables. \n",
    "\n",
    "### Sobre el código\n",
    "\n",
    "Gym contiene un conveniente paquete de *Wrappers* que permiten envolver al *environment* luego de creado con *gym.make('MsPacman-v0')* modificando funciones como *environment.step(action)* para que devuelva un reward modificado desde cualquier otro lado que sea llamada: \n",
    "\n",
    ">    class UnitReward_cs188x(Wrapper):\n",
    ">        def __init__(self, env=None, monitor=None):\n",
    ">            super(UnitReward_cs188x, self).__init__(env)\n",
    ">            \n",
    ">        def step(self, action):\n",
    ">            state, reward, done, info = self.env.step(action)\n",
    ">            if reward > 0:\n",
    ">                reward = 1.\n",
    ">            else:\n",
    ">                reward = 0.\n",
    ">\n",
    ">            return state, reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "Se implementó ***RMSProp con parámetros compartidos*** para contrastar con los de ***Adam***, previamente utilizado.\n",
    "\n",
    "No se observó ninguna diferencia en largos experimentos con los mismos hyperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrada\n",
    "\n",
    "Se modificó el tipo de entrada a la red neuronal.\n",
    "\n",
    "* Manteniendo las **4 rotaciones** de 90°\n",
    "* Asignando a cada rotación, los valores (intensidades) de su **canal de color correspondiente (RGB)**, y *grayscale* para la última imagen.\n",
    "\n",
    "![pacman-episode-4.gif](./img/pacman-episode-4.gif)\n",
    "\n",
    "Originalmente se pensó como alternativa a buscar manualmenmte el mejor equilibrio de grises para la red (variando mean, varianza, etc).\n",
    "\n",
    "Las primeras pruebas fueron usando ***ms-pacman de Atari***, sin las rotaciones (ya que tiene información extra en la parte inferior de la pantalla) pero usando el mismo tipo de división de colores:\n",
    "\n",
    "![mspacman-episode-4.gif](./img/mspacman-episode-4.gif)\n",
    "\n",
    "Partiendo de **pesos ya entrenados** solo con escala de grises, el agente mostró una rápida adaptación al nuevo tipo de entrada:\n",
    "\n",
    "![learning-after-colours.png](./img/learning-after-colours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Frame\n",
    "\n",
    "Otra característica mencionada pero poco explorada en el anterior proyecto, fue el uso de ***skip frame***.\n",
    "\n",
    "La primera impresión al ejecutar ms-pacman con el mismo modelo anterior, fue de que **la simulación también era lenta**.\n",
    "\n",
    "El motivo fue el **alto frame rate**, con el cual cada frame era un estado que debía procesarse por la NN.\n",
    "\n",
    "Para eso se utiliza ***skip frame*** que elije, combina y/o saltea frames consecutivos.\n",
    "\n",
    "Para DQN, la entrada a la red neuronal fueron 4 frames apilados:\n",
    "\n",
    "    [max(T-1,  T),\n",
    "     max(T+3,  T+4),\n",
    "     max(T+7,  T+8),\n",
    "     max(T+11, T+12)]\n",
    "     \n",
    "Donde `max(T-1, T)` es el máximo *elementwise* entre las matrices de píxeles de **dos frames consecutivos**.\n",
    "\n",
    "**Descartando otros 2** frames luego de cada operación max().\n",
    "\n",
    "\n",
    "Para pacman de Atari, se utilizó un filtro similar al de DQN, con una diferencia:\n",
    "\n",
    "* con 1 solo *skip frame*\n",
    "\n",
    "        [max(T,    T+1),\n",
    "         max(T+3,  T+4),\n",
    "         max(T+6,  T+7),\n",
    "         max(T+9,  T+10)]\n",
    "\n",
    "* Sin *skip frames*, solo máximo entre cada dos\n",
    "\n",
    "        [max(T,    T+1),\n",
    "         max(T+2,  T+3),\n",
    "         max(T+4,  T+5),\n",
    "         max(T+6,  T+7)]\n",
    "\n",
    "La siguiente imagen muestra la entrada de píxeles de los 4 estados a la red neuronal, calculando máximo entre cada dos, y salteando 1 frame.\n",
    "\n",
    "![dqn-skip-frame.png](./img/dqn-skip-frame.png)\n",
    "\n",
    "Se determinó que **el frame descartado limitaba la capacidad de reacción del agente**, por lo que se decidió por no usar skip frames.\n",
    "\n",
    "El cambio de entrada produjo resultados negativos (esperados) al agente **ya entrenado** con la entrada anterior.\n",
    "\n",
    "En unas horas de entrenamiento volvió a los valores anteriores y los siguió superando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Reciclaje + *Finetunning*\n",
    "\n",
    "Una vez entrenado el agente de modelo neuronal **lineal**, se decidió probar nuevamente una capa LSTM en su lugar.\n",
    "\n",
    "Para ello:\n",
    "\n",
    "1. Se **cargaron los pesos ya entrenados** en el modelo **lineal**\n",
    "2. **Se *\"congelaron\"* las capas convolucionales** para mantener los filtros/features aprendidos\n",
    "3. **Se entrenó el resto** del modelo neuronal **hasta ver mejoras** en el resultado\n",
    "4. Se descongelaron todas las capas y siguió normalmente con el entrenamiento en conjunto\n",
    "\n",
    "A continuación, el nuevo modelo neuronal con LSTM y las capas convolucionales congeladas siendo entrenado, partiendo de los pesos del modelo lineal:\n",
    "\n",
    "![lstm-learn.png](./img/lstm-learn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least\n",
    "# Clamp Critic (Value Function)\n",
    "\n",
    "En la sección [\"Estrategias de modelado\"](https://github.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-/blob/master/Very%20quick%20roadmap%20to%20Asynchronous%20Advantage%20Actor%20Critic.ipynb#Estrategias-de-modelado) del trabajo anterior, se notaron **picos muy grandes** en el error (*loss*) calculado, por lo que se optó por poner topes en el mismo de $\\frac{1}{\\text{step-size}}$, sin conocer el motivo de ellos.\n",
    "\n",
    "El problema real fue que la **Value Function (*Critic*)** era una salida en la red neuronal que entregaba valores reales, sin cotas de ningún tipo.\n",
    "\n",
    "Recordando el modelo:\n",
    "\n",
    "![actor-critic-model.png](./img/actor-critic-model.png)\n",
    "\n",
    "Dado que el error total se calcula como:\n",
    "\n",
    ">    total_loss = -actor_loss + critic_loss - opt.beta * entropy_loss\n",
    "\n",
    "Donde *critic_loss* es:\n",
    "\n",
    ">    critic_loss = critic_loss + ((Gt - value)\\*\\*2) / 2\n",
    "\n",
    "Esa diferencia al cuadrado era la causante de tales magnitudes.\n",
    "\n",
    "Una solución encontrada, fue **acotar los valores de la value function** entre $\\left[-20,\\, 20\\right]$, siendo los valores que serán comparados con el $G_t$ del episodio en cada comparación (cada step guardado).\n",
    "\n",
    "La magnitud $20$ surge de ver los plots de rewards promedio alcanzando valores entre 20 y 30 casi ganando el episodio.\n",
    "\n",
    "Esta magnitud **será particular de cada juego**, como también **de cada instancia en el tiempo**, distintas para un agente entrenado como para uno random.\n",
    "\n",
    "## Cotas dinámicas\n",
    "\n",
    "Una mejor solución fue determinar la magnitud de la cota, como el máximo/mínimo Gt obtenido hasta el momento.\n",
    "\n",
    "> **Inicia** con **cotas para el Crítico** de $[-1, \\,1]$.\n",
    ">\n",
    "> Al finalizar cada episodio ($n$ steps), **se calcula Gt** de los datos guardados\n",
    ">\n",
    "> $G_t^{(n)} = R_{t+1} + \\gamma\\, R_{t+2}\\, + \\gamma^2\\, R_{t+3} + \\, ... \\, + \\, \\gamma^{n-1} R_{t+n} + \\gamma^{n}\\, \\tilde{V}(S_{t+n})$\n",
    ">\n",
    ">     1. Si (Gt > cotaMax): \n",
    ">\n",
    ">        cotaMax = Gt\n",
    ">\n",
    ">     2. Si (Gt < cotaMin): \n",
    ">\n",
    ">        cotaMin = Gt\n",
    "\n",
    "Luego:\n",
    "\n",
    ">      logits, value = local_model(state)\n",
    ">\n",
    ">      value.clamp(cotaMin, cotaMax)\n",
    "\n",
    "De esta forma, los valores máximo/mínimo del Crítico aumentarán **a medida que se vean puntajes más altos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor estimador: *Exponential Cummulative Average*\n",
    "\n",
    "Calcular valores extremos puede ser **contraproducente** si se observan retornos $G_t$ altos en **instancias iniciales**, donde **la Value Function tiene mucha varianza**.\n",
    "\n",
    "Un **mejor estimador** para las cotas es un **promedio incremental**, que dé valores dependientes de su repetición en el tiempo:\n",
    "\n",
    "$$m_{n+1} = m_n + \\frac{G_{t+1} - m_n}{n+1}$$\n",
    "\n",
    "con **pérdida de memoria** (decaimiento exponencial) para valores antiguos:\n",
    "\n",
    "$$m_{n+1} = \\alpha\\, m_n + (1-\\alpha) \\frac{G_{t+1} - m_n}{n+1} \\,\\, , \\, \\alpha \\in [0,1]$$\n",
    "\n",
    "\n",
    "Se eligió un $\\alpha = 0.495$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "De esta manera, **en cada decisión** del agente, habrá una **estimación del valor del siguiente estado**, siendo un valor entre $[-1,\\,1]$, y un consiguiente *reward* luego de la acción $A_t$ que será $R_{t+1} \\in \\{-1,0,1\\}$.\n",
    "\n",
    "Con estos valores podemos volver a: \n",
    "\n",
    ">    critic_loss = critic_loss + ((Gt - value)\\*\\*2) / 2\n",
    "\n",
    "Y ver que **el valor máximo** que puede tomar será $0 \\leq \\frac{1}{2}(R - \\text{value})^2 \\leq 2$ , pues la diferencia máxima entre $R$ y $\\text{value}$ es $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clamp on loss \n",
    "![loss-before-clamp.png](./img/loss-before-clamp.png)\n",
    "\n",
    "* Clamp on Critic value\n",
    "![loss-after-clamp.png](./img/loss-after-clamp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-22T12:19:18.992Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\n",
      "\n",
      "Getting number of NN inputs/outputs for atari\n",
      "Loading previous weights for atari... Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jack/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C/gym_pacman')\n",
    "\n",
    "from train import train\n",
    "from argparse import Namespace\n",
    "args = Namespace(beta =0.001,\n",
    "                 gamma=0.9,\n",
    "                 tau=1.0,\n",
    "                 lr=1e-4,\n",
    "                 layout='atari',\n",
    "                 load_previous_weights=True,\n",
    "                 num_processes=6,                # numero de procesos asincronicos\n",
    "                 num_processes_to_render=1,     # numero de procesos a visualizar\n",
    "                 use_gpu=False,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e9,\n",
    "                 num_local_steps=50,     # async updates every\n",
    "                 save_interval=25,\n",
    "                 saved_path='trained_models',\n",
    "                 log_path='tensorboard',\n",
    "                 record=False)\n",
    "print(\"Agent progress details are displayed on this jupyter notebook TERMINAL (not here)\\n\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:08:26.256145Z",
     "start_time": "2019-08-19T19:08:26.246594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
